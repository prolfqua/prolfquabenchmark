---
title: prolfqua&colon; A Comprehensive R-package for Proteomics Differential Expression Analysis
author: 
  - Witold E. Wolski $^{\dagger\ddagger}$^[[Functional Genomics Center Zurich (FGCZ)](https://fgcz.ch) - [University of Zurich](https://uzh.ch)/[ETH Zurich](https://ethz.ch), Winterthurerstrasse 190, CH-8057 Zurich, Switzerland. $^\ddagger$[Swiss Institute of Bioinformatics (SIB), Quartier Sorge - Batiment Amphipole, 1015 Lausanne, Switzerland](https://www.sib.swiss/) $^\dagger$Correspondence&colon; \texttt{wew@fgcz.ethz.ch}]
  - Paolo Nanni$^{\ast}$
  - Jonas Grossmann$^{\ast\ddagger}$
  - Maria d'Errico$^{\ast\ddagger}$
  - Ralph Schlapbach$^{\ast}$
  - Christian Panse$^{\ast\ddagger}$
output: 
  bookdown::pdf_document2:
      toc: true
      toc_appendix: false
      toc_bib: false
      keep_tex: true
date: "`r format(Sys.time())`"
bibliography: bibliography.bib
vignette: >
  %\VignetteIndexEntry{prolfqua&colon; A Comprehensive R-package for Proteomics Differential Expression Analysis} 
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
abstract: |
    Mass spectrometry is widely used for quantitative proteomics studies, relative protein quantification, and differential expression analysis of proteins. Nevertheless, there is a need for a flexible and easy-to-use application programming interface in R that transparently supports a variety of well principled statistical procedures. The `r BiocStyle::Githubpkg("fgcz/prolfqua")` package can model simple experimental designs with a single explanatory variable and complex experiments with multiple factors and hypothesis testing. It integrates essential steps of the mass spectrometry-based differential expression analysis workflow: quality control, data normalization, protein aggregation, statistical modeling, hypothesis testing, and sample size estimation. The application programmer interface strives to be clear,  predictable, discoverable, and consistent to make proteomics data analysis easy and exciting. Furthermore, the package implements benchmark functionality that can help to compare data acquisition, data preprocessing, or data modeling methods using a gold standard dataset. Finally, we show that the implemented methods allow sensitive and specific differential expression analysis. The `r BiocStyle::Githubpkg("fgcz/prolfqua")` R package is available on GitHub https://github.com/fgcz/prolfqua, distributed under the MIT licence, and runs on all platforms supported by the [R free software environment for statistical computing and graphics](https://www.r-project.org/).
urlcolor: blue
---


```{r setup, include=FALSE}
## See ACS guidelines [https://publish.acs.org/publish/author_guidelines?coden=ancham#manuscript_text_components]
knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      fig.width = 7,
                      fig.height = 5)
ggplot2::theme_set(ggplot2::theme_classic())

options(width=70)
```

\newpage

# Introduction

\begin{quote}
To paraphrase provocatively, `machine learning is statistics minus any checking of models and assumptions'.

\begin{flushright}
-- Brian D. Ripley, useR! 2004, Vienna.
\end{flushright}
\end{quote}

Proteins carry out most crucial functions and give structure to living cells. Hence, measuring changes in protein abundance is the subject of active research [@vidova2017review]. Bottom-up mass spectrometric methods, where proteins are specifically and reproducibly digested into protein fragments - peptides, are employed to identify and quantify proteins in complex samples containing hundreds to thousand of proteins [@Bubis2017; @da2020philosopher]. The peptides are first separated by their chemical and physical properties using liquid chromatography (LC). Afterward, they are ionised, weighed, identified, and quantified using mass spectrometric techniques, e.g., electro-spray-ionization mass spectrometry (ESI-MS). Finally, peptide identification is achieved by fragmenting and matching the measured fragment masses to theoretical masses computed from known peptide sequences [@eng2015deeper; @yu2016pipi; @kong2017msfragger]. For quantification, intact peptide ions [@yu2020fast; @Cox2008MaxQuant] or products of peptide ion fragmentation [@rost2014openswath; @demichev2020dia] are counted and aggregated to obtain peptide abundances. Finally, the identified and quantified peptides are assigned to proteins based on protein sequence information.

Proteomics quantification experiments enable monitoring relative abundances of thousands of proteins in biological samples. Most studies use parallel-group designs, where one or many treatment groups are compared to the control group [@de2022apoe2; @laubscher2021baf]. More recently, more complex experimental designs with an increasing number of samples are studied, e.g., factorial designs and longitudinal studies (time series), sometimes with repeated measurements on the same subject [@tan2022proteomic; @meier2021reduced]. The data can be modeled using linear fixed-, mixed-, or random-effects models [@Bates2015FittingJSS]. Based on these models, tests can be applied to examine whether specific factors and factor interactions are significant, e.g., it can be tested if differences in protein abundance between groups are statistically significant. 

An important aspect of mass spectrometric data are missing peptide and protein quantifications. @rubin1976inference classified missing data problems into three categories: missing completely at random (MCAR), missing at random (MAR), and missing not at random (MNAR). For instance, in data-dependent acquisition (DDA) MS, only a limited number of MS1 signals are selected for fragmentation and identified. Peptide quantification algorithms transfer identification information between MS1 features in different samples by aligning the data using retention time and mass information, thus reducing the amount of missing data. Nevertheless, highly abundant proteins can suppress the detection of other proteins in a sample, a MAR mechanism. Furthermore, a weak correlation between the number of missing measurements in a group and the abundance of the quantified protein can be observed, which is caused by the limit of detection (LOD), a MNAR mechanism [@mcgurk2020use].

Several data analysis packages exist to model mass spectrometry protein quantification experiments, e.g., `r BiocStyle::Biocpkg("limma")` [@Ritchie2015], `r BiocStyle::Biocpkg("MSstats")` [@Choi2014], `r BiocStyle::Biocpkg("PECA")` [@Suomi2017bEnhanced],  `r BiocStyle::Biocpkg("msqrob2")` [@Goeminne2016] or `r BiocStyle::Biocpkg("proDA")` [@bioxrvproDA2020], to mention some, all implemented in R. Each of them has some unique features, for example, `r BiocStyle::Biocpkg("MSstats")` infers the model and generates the model formula from the sample annotation, allowing users with limited statistical knowledge to perform differential expression analysis (DEA). At the same time, `r BiocStyle::Biocpkg("limma")` allows to specify a design matrix using a linear model formula and implements the empirical Bayes variance shrinkage method. The package `r BiocStyle::Biocpkg("PECA")`, performs a roll-up of peptide level differences and peptide level $p$-value estimates, obtained from `r BiocStyle::Biocpkg("limma")` or `r BiocStyle::Biocpkg("PECA")`, to protein level estimates. Furthermore, `r BiocStyle::Biocpkg("msqrob2")` implements two models: robust linear models fitted to protein intensities and robust ridge regression fitted to peptide intensities, and combines them with empirical Bayes variance shrinkage. The `r BiocStyle::Biocpkg("proDA")` package implements a linear probabilistic dropout model to account for missing data without imputation. `r BiocStyle::Biocpkg("MSstats")` handles missing data by feature filtering and using imputation. Other means of modelling missing observations are the Hurdle model discussed by @goeminne2020msqrob, while the R package `r BiocStyle::Biocpkg("proDA")` models missingness using probabilistic dropout models [@bioxrvproDA2020].

When analyzing parallel-group designs using a single explanatory variable, and contrasting groups, we can use all R packages; but, we can use only some of them if we want to model factorial designs or repeated measurements. Table \@ref(tab:tableOverview) gives an overview of the models and features supported by these packages. We see that packages such as `r BiocStyle::Biocpkg("limma")` and `r BiocStyle::Biocpkg("proDA")` allow us to fit a comprehensive variety of models and test various hypotheses; however, in-depth knowledge of design matrix specification using the R formula interface is required [@law2020guide].

```{r tableOverview}
xx <- list(
  MSstats = c("pd" = "Y", "rm" = "Y", "mem" = "Y"),
  ROPECA = c("pd" = "Y", "rm" = "Y*", fd = "Y", int = "Y", eb = "Y"),
  limma  = c("pd" = "Y", "rm" = "Y*", fd = "Y", int = "Y",eb = "Y"),
  MSqRob2_rlm = c("pd" = "Y", "rm" = "Y*", fd = "Y", int = "Y",eb = "Y", md = "?"),
  proDA = c("pd" = "Y", "rm" = "Y*", fd = "Y", int = "Y", eb = "Y", md = "Y"),
  prolfqua = c("pd" = "Y", "rm" = "Y", fd = "Y", int = "Y", mem = "Y",eb = "Y", md = "Y"))

bb <- data.frame(dplyr::bind_rows(xx))
rownames(bb) <- names(xx)
knitr::kable(data.frame(bb), caption = "Rows - R package, Columns - types of models supported: pd - parallel design, rm - repeated measurements, fd - factorial design, int - interactions among factors, mem - mixed effect models, eb - empirical Bayes, md - missing data modelling. Y - yes, * - repeated measurements are modeled using a fixed effect. ? - the hurdle model was published but is not available in the msqrob2 package.")
```

When developing the R package `r BiocStyle::Githubpkg("fgcz/prolfqua")` we were inspired by the R package `r BiocStyle::CRANpkg("caret")` [@caret2008], which enables us to call various machine learning (ML) methods, and makes selecting the best ML algorithm for your problem easy. We aimed for a similar R package for the differential expression analysis of proteomics data. However, after examining the R packages for modeling proteomics quantification data, we observed that model specification, input, and output formats differ widely. At the same time, they have in common that: they fit linear models either to peptide or protein intensities, determine differences among groups, and afterward apply empirical Bayes variance shrinkage. Therefore, the revised goal was to provide a modular object-oriented (OO) design, with R linear models as a core, where we can add features such as $p$-value aggregation, e.g., `r BiocStyle::Biocpkg("PECA")`, variance shrinkage, or modeling of missing observations.

Furthermore, the functionality of `r BiocStyle::Githubpkg("fgcz/prolfqua")` also includes methods specific to proteomics data. For example, we implemented strategies to estimate protein intensities from peptide intensities: top N [@Grossmann2010], Tukey's median polish  [@tukey1977exploratory], robust linear models [@goeminne2020msqrob]. Furthermore, peptide or protein abundances can be scaled and transformed, using robust scaling, _quantile_ normalization or `r BiocStyle::Biocpkg("vsn")` to remove systematic differences among samples and heteroscedasticity. In this respect, `r BiocStyle::Githubpkg("fgcz/prolfqua")` can be compared with other R packages such as `r BiocStyle::Biocpkg("DEP")` [@DEP2018] or `r BiocStyle::Biocpkg("POMA")` [@POMA2021] which support the entire differential expression analysis pipeline. 

We also implemented functionality and use the Ionstar [@shen2018ionstar] dataset to benchmark the modeling methods implemented within `r BiocStyle::Githubpkg("fgcz/prolfqua")` and to compare our results with those of `r BiocStyle::Biocpkg("MSstats")` and `r BiocStyle::Biocpkg("proDA")`. Since group sizes are relatively small, typically with four or five subjects per group, the high power of the tests is a relevant criterion to assess the modeling method. The quantified proteins can be ranked using the estimated fold-change, $t$-statistics, or scaled $p$-value, and afterward subjected to gene set enrichment (GSEA) or over-representation analysis [@subramanian2005gene] to determine up or down-regulated groups of proteins. Furthermore, the statistical model must provide an unbiased estimate of the false discovery rate (FDR) to manage expectations when selecting protein lists for follow-up experiments. We will use the partial area under the receiver operator curve (ROC) to assess the power of the tests and compare the FDR with the false discovery proportion (FDP).

\newpage
# Methods

## Implementation

We store all the data needed for analysis in a tidy table, i.e., every column is a variable, every row is an observation, and every cell is a single value [@tidydata2014]. Using an R6 [@R6cite] configuration object (Figure \@ref(fig:LFQData)), we specify what variable is in which column, making it easy to integrate new inputs in `r BiocStyle::Githubpkg("fgcz/prolfqua")` if provided as a tidy table. For example, to visualize tidy Spectronaut[@bruderer2015extending], DiaNN[@demichev2020dia], or Skyline[@maclean2010skyline] outputs, or data in `r BiocStyle::Biocpkg("MSstats")`[@Choi2014] format, only a few lines of code are needed to update the `r BiocStyle::Githubpkg("fgcz/prolfqua")` `AnalysisTableConfiguration` configuration. The configuration encapsulates the differences in column names among the various input formats, and enables the use of the methods without additional arguments. We show an example code for creating an MSFragger[@yu2020fast] configuration in the Appendix. For popular software like MaxQuant[@Cox2008MaxQuant], or MSFragger, which stores the same variable, e.g., intensity, in multiple columns, one for each sample, we implemented methods that transform the data into tidy tables. Relying on the tidy data table enables us to interface with many data manipulation, visualization, and modeling methods, implemented in base R [@RCoreTeam2021] and the tidyverse [@tidyverse2019], easily. We use R6 classes to structure the functionality of the package (see Figure \@ref(fig:LFQData) and Figure \@ref(fig:ContrastUML)). R6 classes are well supported by command-line completion features in RStudio [@RStudio2022], and help to implement argument free functions (Figure \@ref(fig:tabCompletion).

(ref:LFQData) Class Diagram of classes representing the proteomics data. The LFQData class encapsulates the quantitative proteomics data stored in a tidy table. An instance of the AnalysisTableConfiguration class specifies a mapping of columns in the tidy table. The LFQDataPlotter class and other classes decorate the LFQData class with additional functionality. Similarly, the `LFQDataStats` and `LFQDataSummary` reference the `LFQData` class and group methods for variance and sample size estimation or summarizing peptide and protein counts.

```{r LFQData, echo=FALSE, fig.cap="(ref:LFQData)", out.width = '90%'}
knitr::include_graphics("LFQData_UML_V2.png")
```

In addition, we implement features specific to high throughput experiments, such as the experimental Bayes variance and $p$-value moderation, which utilizes the parallel structure of the protein measurements and the analysis [@Ritchie2015]. We also compute probabilities of differential protein regulation based on peptide level models [@Suomi2017bEnhanced]. We used R6 classes to encapsulate the statistical modelling functionality in `r BiocStyle::Githubpkg("fgcz/prolfqua")` (see Figure \@ref(fig:ContrastUML)). We specify a contrast interface (`ContrastsInterface`). Several implementations allow to perform differential expression analysis given linear or mixed effect models (`Contrasts`), variance shrinkage (`ContrastsModerated`), or to impute contrasts in cases when observations are missing for an entire group of samples (`ContrastsSimpleImpute`). Further implementations of the interface encapsulate and integrate differential expression analysis results of external tools such as `r BiocStyle::Biocpkg("proDA")` or of SAINTexpress [@teo2014saintexpress] used to analyze data from protein interaction studies.

(ref:ContrastUML)  The UML (unified modeling language) diagram of modeling and contrast related classes. Different strategies, e.g., _lm_, _lmer_, and _glm_ (Table \@ref(tab:prolfquaModels)), can fit various types of models. The model builder method fits the statistical model given the data and a strategy. The obtained model can then be used for the analysis of variance or to estimate contrasts. All classes estimating contrasts implement the _Contrasts_ interface. Results of external tools, e.g., SAINTexpress, can be adapted by implementing the Contrasts Interface.


```{r ContrastUML, echo=FALSE, fig.cap="(ref:ContrastUML)", out.width = '90%'}
#fig_svg <- cowplot::ggdraw() + cowplot::draw_image("ContrastClassesUML.svn")
#plot(fig_svg)

knitr::include_graphics("ContrastClassesUML.png")
```


## Dataset for benchmarking

To evaluate the performance of differential expression analysis, we use the IonStar benchmark dataset[@shen2018ionstar], available from the Proteomics Identifications Database (PRIDE) identifier PXD003881. $DH5\alpha$ _E. coli_ lysate was spiked in human pancreatic cancer cells (Panc-1) lysate at five different levels: $3\%$, $4.5\%$, $6\%$, $7.5\%$ and $9\%$ _E. coli_. We annotated these dilutions from smallest to largest with the letters _a_, _b_, _c_, _d_, _e_. By comparing the various dilutions, we obtain different effect sizes, e.g., when comparing dilution _e_ (9%) against dilution _d_ (7.5%), the expected difference is $1.2$ for E. coli proteins and $1$ for human proteins. There are four technical replicates for each dilution, and hence 20 raw files in total. We processed the raw data using MaxQuant [@Cox2008MaxQuant] version Version 1.6.10.43, with MaxQuant default settings. The text files generated by MaxQuant are available in the _prolfquadata_ R package [@prolfquadata]. To compare the performance of the various methods implemented in `r BiocStyle::Githubpkg("fgcz/prolfqua")` we use only the contrasts resulting in minor differences $\Delta = (1.20, 1.25, 1.30, 1.50)$, because for bigger differences all models perform similarly.

## Data preprocessing for model comparison

The peptide abundances (from the MaxQuant _peptide.txt_ file) were $log_2$ transformed and subsequently scaled, where median and the mean average deviation (mad) were obtained from the human proteins only. We removed _one hit wonders_, i.e., proteins with a single peptide assignment. Protein abundances are inferred from the peptide intensities using Tukey's median polish. Finally, we fitted the fixed effect linear models, the dropout model `r BiocStyle::Biocpkg("proDA")` to protein abundances, the mixed effect linear model, and the `r BiocStyle::Biocpkg("PECA")` model to peptide intensities.

## Benchmark metrics

The IonStar dataset contains _H. sapiens_ proteins with constant concentrations and _E. coli_ proteins with varying concentrations. We know that for _H. sapiens_ proteins, the difference $\beta$ between two dilutions should be $\beta = 0$, while for _E. coli_ proteins, we know that the difference between dilutions should be $\beta \ne 0$.

We can use various statistics to examine the alternative hypothesis $\beta \ne 0$: the contrast estimate, i.e. the $\log_2$ fold-change $\beta$, the $t$-statistic $\frac{\beta}{\sqrt{var(\beta)}}$, or the $p$-value and moderated $p$-value. For each statistic and each value of the statistics we then compute a confusion matrix (see Table \@ref(tab:knitrConfusionMatrix)). From the confusion matrix we obtain measures such as true positive rate ($TPR$), false positive rate ($FPR$), or false discovery proportion ($FDP$) which are given by:

```{r knitrConfusionMatrix}
table <- data.frame( c("beta != 0", "beta == 0", "Total"),
                     matrix(c("TP", "FP", "R", "FN", "TN", "", "P", "N", "m"),
                            ncol = 3, byrow = T))
colnames(table) <- c("Prediction \\ Truth","E.coli", "H.sapiens", "Total")

knitr::kable(table, caption = "Confusion matrix, TP - true positive, FP - false positive, FN - false negative, TN - true negatives, P - all positive cases (all $E.~coli$ proteins), N - all negative cases (all $H.~sapiens$ proteins), m - all proteins.")
```

with

\begin{eqnarray}
TPR &= \frac{TP}{TP+FN} = \frac{TP}{P}\\
FPR &= \frac{FP}{FP+TN} = \frac{FP}{N}\\
FDP &= \frac{FP}{TP + FP} = \frac{FP}{R}
\end{eqnarray}

By plotting the $TPR$ versus the $FPR$ we obtain the receiver operator characteristic curve (ROC curve). The area under the curve (AUC) or partial areas under the curve (pAUC), at various values of the $FPR$, are measures of performance derived from the ROC curve. By using these measures we can compare the performances of the statistics produced by the various methods examined.

In order to compute the confusion matrices based on the $p$-value we first need to rescale it (see Equation \@ref(eq:scalepvalue)). Thus, the $p$-value is a function of the $t$-statistics and the degrees of freedom. 

## Modelling

### Robust scaling of the data

@valikangas2016systematic discuss and benchmark various methods of peptide or protein intensity normalization, such as variance stabilizing normalization [@huber2002variance] or quantile normalization [@bolstad2003comparison]. In this work, we use a robust version of the z-score, where instead of the mean $\bar{x}$ we use the median, and instead of the standard deviation $\tilde{S}$ we use the median absolute deviation (mad):

\begin{eqnarray}
z = \frac{x - \tilde{x}}{\tilde{S}}
\end{eqnarray}

Because we need to estimate the differences among groups on the original scale, we must multiply the $z$-score by the average variance of all the $n$ samples in the experiment.

\begin{eqnarray} 
z' = z \cdot \frac{1}{n}\sum_{i=1}^n S_i
\end{eqnarray}

To apply this transformation, we need to estimate two parameters per sample, therefore it works for experiments with thousands of proteins and experiments where only a few hundred proteins per sample are measured.
For the Ionstar dataset, we used the intensities of _H. sapiens_ proteins, whose concentrations do not change, to determine $\bar{x}$ and $S$ and then applied it to all the intensities (including _E. coli_) in the sample.


### Estimating differences between groups

Given a linear model $y = \beta X$, we can compute the difference $\beta_{c}$ between two groups by a linear combination $c$ of linear model parameters $\beta$, where $c$ is a column vector with as many elements as there are coefficients $\beta$ in the linear model. If $c$ has $0$ for one or more of its rows, then the corresponding coefficient in $\beta$ is not involved in determining the contrast [@ph525xseries].

The difference estimate $\beta_c$, is given by the dot product:

\begin{eqnarray} 
\hat{\beta_{c}} &=& c^T \beta 
\end{eqnarray} 

and the variance of $\beta_c$ by:

\begin{eqnarray} 
\textrm{var} (\hat\beta_c) &=& \sqrt{ c^T \sigma^2 (X^T X)^{-1} c }
\end{eqnarray}

with $X$ being the design matrix. The degrees of freedom for the contrast are equal to the residual degrees of freedom of the linear model. For estimating contrasts from mixed effects models we used the function `contest` implemented in the R package `r BiocStyle::CRANpkg("lmerTest")`  and used the Satterthwaite [@Kuznetsova2017lmerTest] method to estimate the denominator degrees of freedom. These methods are available in the class `Contrast` (see Figure \@ref(fig:ContrastUML))

### Determining linear parameter combinations for treatment comparison

The package `r BiocStyle::Githubpkg("fgcz/prolfqua")` provides a function to determine the vector of $parameter$ weights $c$, from a linear models and a contrast specification string.

The linear model below explain the observed protein abundances using the explanatory variables `factor_1` and `factor_2` and the interaction among them `factor_1:factor_2`, 

```{r}
Intensity ~ factor_1 + factor_2 + factor_1:factor_2
```

then the contrasts among `group_1` and `group_2` defined by `factor_1` can be specified using the string below.

```{r eval=FALSE, echo=TRUE}
c("contrast_name" = "factor_1group_1 - factor_1group_2")
```

Furthermore, contrasts for subgroups can be specified using the code below,

```{r eval=FALSE, echo=TRUE}
c("contrast_name" =
    "`factor_1group_1:factor_2group_a` - `factor_1group_1:factor_2group_b`")

```

where `factor_x` is the name of the explanatory variable, and `group_x`  are group labels. 

The following code shows an example where we specify two contrast: the first to compare Cells of type CMP/MEP with cells of type HSC, and the second to compare therapy NO with therapy NU for the celltype CMP/MEP [@meier2021reduced]. Finally, we compute the array of weights defining the contrast, used to multiply the model coefficient $\beta$.


```{r echo=TRUE}
Contrasts <- 
    c("CMP/MEPvsHSC" = "`CelltypeCMP/MEP` - `CelltypeHSC`",
   "NOvsHU" = "`class_therapyc.NO:CelltypeCMP/MEP` - `class_therapyp.HU:CelltypeCMP/MEP`")

m <- prolfqua::prolfqua_data('data_basicModel_p1807')
linfct <- prolfqua::linfct_from_model(m,as_list = FALSE)

prolfqua::linfct_matrix_contrasts(linfct, Contrasts )
```



### Contrast estimation in presence of missing data.

Missing observations lead to different group sizes, which results in unbalanced designs. Linear and mixed effect models can handle unbalanced designs, as long as one observation per group is available, they will produce unbiased estimates, and no imputation is needed.

If there is no observation in a group, we assume that the protein is unobserved because the protein abundance is below the limit of detection (LOD) of the MS instrument. In this case, we will impute the mean using the protein abundance at the detection limit $A_{LOD}$.
We estimate the abundance at the detection limit using the abundances of the proteins observed only in one sample of a group of samples. Then, we compute the median of the distribution and use it as the group mean if a protein is absent in a treatment group.

When computing differences $\Delta$ among two groups $a$ and $b$, we will use the group mean $\bar{a}$ or $\bar{b}$ estimated from the data, or if no observations are present in a group, we use $A_{LOD}$, e.g., If there are no observations in group $b$ then :

$$
\Delta = 
\begin{cases}
\bar{a} - A_{LOD} & \text{if} ~~ \bar{a} > A_{LOD}\\
0  &\text{if} ~~ \bar{a} < A_{LOD}.
\end{cases}
$$


To estimate the variance, we assume that the variance of the protein is constant in all the groups and use the pooled variance based on all data: 

\begin{equation} 
s_p^2=\frac{\sum_{i=1}^k (n_i - 1)s_i^2}{\sum_{i=1}^k (n_i - 1)}
\end{equation} 

with $n_i$ the number of observations, and $s_i$ the standard deviation in each group.
The standard deviation for the $t$-statistics is then given by:

\begin{equation} 
s = \sqrt{\frac{2n_g s_p^2}{n}},
\end{equation} 

Where, $n_g$ is the number of groups and $n$ is the number of observations.
If variance can not be estimated for a protein, we will use the median pooled variance of all the proteins.

This methods are implemented in the class `ContrastSimpleImpute` (see Figure \@ref(fig:ContrastUML)).

### $p$-value moderation

From the linear and the mixed effect models, we can obtain the residual standard deviation $\sigma$, and degrees of freedom $df$. @Smyth2004linear discuss how, to use the $\sigma$ and $df$ of all models to estimate a prior $\sigma$ and prior $df$, and posterior $\tilde \sigma$. These can be used to moderate the $t$-statistics by:

\begin{equation} 
\tilde{t}_{pj} = \frac{t_{pj} s_p}{\tilde{s}_p}.
\end{equation} 

We implemented this method in the class `ContrastModerated` (Figure \@ref(fig:ContrastUML)).

### Summarizing peptide level differences and p-values on protein level

To summarize peptide level models to protein models, we apply the method suggested by @Suomi2017bEnhanced that use the median scaled $p$-value of the peptide models and the cumulative distribution function of the Beta distribution (CDF) to determine a regulation probability of the protein.

To obtain the $\tilde{p}$ of a protein, we first rescaled the peptide $p$-values by taking the sign of the fold-change $\hat \beta$ into account, i.e.:

\begin{equation}
p_{s} =
  \begin{cases}
1-p, & \textrm{if}~ \hat{\beta} > 0\\
p-1, & \textrm{otherwise}
\end{cases}
(\#eq:scalepvalue)
\end{equation}

Afterwards, the median scaled $p$-value $\tilde{p}_s$ is determined and using the transformation below, transformed back onto the original scale:

\begin{equation}
\tilde{p} = 1 - |\tilde{p}_{s}|
\end{equation}

Because we use the median, with the i-th order statistic $i = \frac{n}{2} + 0.5$, we parametrize the CDF of the Beta distribution with $\gamma = i = \frac{n}{2} + 0.5$ and $\delta = n - i + 1 = n - (\frac{n}{2} + 0.5) + 1 = \frac{n}{2} + 0.5 = \gamma$. We implemented this method in the class `ContrastROPECA` (Figure \@ref(fig:ContrastUML)).


# Results and Discussion.


## Example analysis workflow

The code snippets in this section demonstrate how a differential expression analysis workflow can be implemented using the `r BiocStyle::Githubpkg("fgcz/prolfqua")` R package. To speed up the computation of these examples, we use a random subset of the Ionstar dataset containing $163$ proteins and $1258$ peptides. Peptide abundances are $\log_2$ transformed and robust z-score scaled using the method `robscale`. Using the `LFQDataPlotter` class, we show the distribution of the normalized peptide abundances in Figure \@ref(fig:prepro) Panel A. Afterwards, protein intensities are estimated from peptide intensities using Tukey's median polish. Figure \@ref(fig:prepro) Panel B shows the peptide intensities and the estimated protein intensities. Next, we compute the standard deviation of all the proteins in each group and display their distribution using violin plots (Panel C). Finally, we create a boxplot (Panel D) showing the abundance of one protein.

```{r prepro,  echo = TRUE , fig.cap="(ref:scaling)", out.width = '90%', fig.width = 8, fig.height = 8}
## load peptide level data
d <- prolfqua::prolfqua_data('data_ionstar')$filtered()

## create R6 obejct
lfqd <- prolfqua::LFQData$new(d$data, d$config) 

##  transform intensities
t <- lfqd$get_Transformer()

lfqd <-  t$log2()$robscale()$lfq
lfqd$rename_response("peptide_abundance")

## infer protein intensities from peptide intensity
agr <- lfqd$get_Aggregator()
lfqp <- agr$medpolish()
lfqp$rename_response("protein_abundance")

## plot panels A-D
pl <- lfqd$get_Plotter()
panelA <- pl$intensity_distribution_density() +
  ggplot2::labs(tag = "A") + ggplot2::theme(legend.position = "none")
panelB <- agr$plot()$plots[[1]] + ggplot2::labs(tag = "B")
panelC <- lfqp$get_Stats()$violin() + ggplot2::labs(tag = "C")
pl <- lfqp$get_Plotter()
panelD <- pl$boxplots()$boxplot[[1]] + ggplot2::labs(tag = "D")
ggpubr::ggarrange(panelA, panelB, panelC, panelD)
```

(ref:scaling) Panel A - Peptide intensity distributions for $20$ samples. For each sample a line with a different colour is shown. Panel B - Peptide intensities for protein _5NTD_ are shown using a line of different colour, and the protein intensity estimate is shown using a fat black line, Panel C - distribution of standard deviations of all proteins in each dilution group ($a$, $b$, $c$, $d$, $e$) and overall (all), Panel D - Distribution of protein intensities for protein _5NTD_.

The following code example illustrates how we compute differences among groups. First, the linear model and the differences are specified. Afterward, the model is fitted to the data using the `build_model` function. Next, we estimate the contrasts either from the linear model using the `Contrasts` class or directly from the data using the `ContrastsSimpleImpute` class. Afterward, we apply $t$-statistic moderation using the `ContrastModerated` class. Finally, the `addContrastResults` function merges both sets of contrast estimates, preferring the one obtained from the linear model if both are available. Then we create the plots shown in Figure \@ref(fig:exampleContrasts). Panel A shows the distribution of the $p$-values, while Panel B shows the volcano plot for each comparison.

(ref:exampleContrasts) Panel A - Histogram showing the distribution of p-values for $163$ proteins. Panel B -  Volcano plot showing $-\log_{10}$ transformed FDR as function of the difference between groups for $163$ proteins.

```{r exampleContrasts, fig.cap="(ref:exampleContrasts)", echo=TRUE, out.width = '90%', fig.width = 8, fig.height = 5}
# specify differences among groups
contrasts <- c(
  "dilution_(9/7.5)_1.2" =   "dilution.e - dilution.d",
  "dilution_(7.5/6)_1.25" =   "dilution.d - dilution.c"
)
## fit model
lmmodel <- paste(lfqp$intensity_column()," ~ dilution.")
modelFunction <- prolfqua::strategy_lm( lmmodel, model_name = "lm")
models <- prolfqua::build_model(lfqp, modelFunction)

## compute contrasts from linear model, moderate t-statistics and p-values
contr <- prolfqua::Contrasts$new(models, contrasts) |> 
  prolfqua::ContrastsModerated$new()

# compute contrasts using imputation imputation and moderate
conI <- prolfqua::ContrastsSimpleImpute$new( lfqp, contrasts) |> 
  prolfqua::ContrastsModerated$new()

# merge contrasts, to obtain differences for all proteins
contrasts <- prolfqua::addContrastResults(prefer = contr, add = conI)

## visualize results in panels
pl <- contrasts$merged$get_Plotter() 
panelA <- pl$histogram()$p.value + ggplot2::labs(tag = "A")                                                        
panelB <- pl$volcano()$FDR +
   ggplot2::theme(legend.position = "bottom") +
   ggplot2::labs(tag = "B")


gridExtra::grid.arrange(panelA, panelB, ncol = 2)
```

R linear model and linear mixed effect models allow modeling parallel designs, repeated measurements, factorial designs, and many more. 
Models in `r BiocStyle::Githubpkg("fgcz/prolfqua")` are specified using R’s linear model and mixed model formula interface. Therefore, knowledge of the R regression model infrastructure [@faraway2016extending; @MASS2002] is advantageous when using our package. Furthermore, this glass box approach should make it easy to reimplement an analysis performed with `r BiocStyle::Githubpkg("fgcz/prolfqua")` using base R or other programming languages by reading the analysis script. However, acknowledging the R formula interface’s complexity to non-statisticians and the popularity of `r BiocStyle::Biocpkg("MSstats")`, we provide the functionality to suggest a model formula from the sample annotation provided in tabular form similarly to `r BiocStyle::Biocpkg("MSstats")`.

Using the tidy table to model the data ensures interoperability with other proteomics-related packages that manage their data with tidy-tables, e.g., `r BiocStyle::CRANpkg("protti")` [@quast2022protti]. The use of R6 classes, which encapsulate the configuration and the data, allows for writing very concise code (no function arguments needed). Auto-completion support in the Rstudio editor makes it easy for novices to explore `r BiocStyle::Githubpkg("fgcz/prolfqua")`s functionality (Figure \@ref(fig:tabCompletion)). To simplify integration of `r BiocStyle::Githubpkg("fgcz/prolfqua")` with Bioconductor based workflows there is a method to convert the LFQData class into a `r BiocStyle::Biocpkg("SummarizedExperiment")`.

To ease the usage barriers of the R package to users not being familiar with statistics and R programming, we integrated some workflows into our data management platform [b-fabric](https://fgcz-bfabric.uzh.ch/) [@bfabric]. This integration enables our users to select the input and basic settings in a graphical user interface. Then, as an output, the user receives a report including input files, the R markdown file, and R scripts necessary to replicate the analysis using their in-house R installation.
The b-fabric system runs a computing infrastructure controlled by a local resource management system that supports cloud-bursting [@VMMAD].
In this way, [b-fabric](https://fgcz-bfabric.uzh.ch/) helps scientists to meet requirements from funding agencies, journals, and academic institutions to publish data according to the FAIR (Findable, Accessible, Interoperable and Reusable) [@FAIR] data principles.

## Benchmarking modelling approaches

The Benchmark functionality of `r BiocStyle::Githubpkg("fgcz/prolfqua")` includes receiver operator curves (ROC) and computes partial areas under those curves (pAUC) and other scores, e.g., the false discovery proportion FDP. We use those scores, i.e. the $pAUC$ at $10%$ FDR and the FDP, to examine how well the methods implemented in `r BiocStyle::Githubpkg("fgcz/prolfqua")` model quantitative mass spectrometric high throughput data and compare them with results produced by `r BiocStyle::Biocpkg("MSstats")` and `r BiocStyle::Biocpkg("proDA")`. Table \@ref(tab:describeBenchmarkedModels) summarizes all modelling methods we evaluated.


```{r readBenchmarkData}

getpath <- function(filN){
    f1 <- paste0("../../inst/Benchresults/",filN)
    if(f1 == ""){
        f1 <- system.file("Benchresults",filN, package = "prolfquabenchmark")
    }
    message(f1)
    return(f1)
}
allBenchmarks <- readRDS(getpath("allBenchmarks.RDS"))
benchmark_msstats <- readRDS(getpath("benchmark_msstats.RDS"))
benchmark_prodA <- readRDS(getpath("benchmark_medpolish_proDA.RDS"))
msFragger <- readRDS(getpath("MSFragger_medpol_benchmark.RDS"))


allBenchmarks$benchmark_mssstats <- benchmark_msstats
allBenchmarks$benchmark_msFragger <- msFragger
allBenchmarks$benchmark_proDA <- benchmark_prodA
allBenchmarks <- allBenchmarks[c("benchmark_imputation","benchmark_ProtModerated",  "benchmark_mixedModerated", "benchmark_ropeca","benchmark_merged","benchmark_mssstats","benchmark_proDA"   )]

```


```{r benchmarkROC}
ttt <- sapply(allBenchmarks, function(x){x$complete(FALSE)})

res <- purrr::map_df(allBenchmarks, function(x){x$pAUC()})


res <- res |> dplyr::mutate(whatfix = dplyr::case_when(what == "scaled.beta.based.significance" ~ "scaled.p.value", TRUE ~ what))

norm <- res |> dplyr::group_by(contrast,whatfix) |>
    dplyr::summarize(meanpAUC_10 = mean(pAUC_10))
res <- dplyr::inner_join(res, norm)
res <- dplyr::mutate(res , pAUC_10n = pAUC_10 - meanpAUC_10)

resAllB <- res |> dplyr::filter(contrast == "all")

p1_pAUC <- ggplot2::ggplot(resAllB, ggplot2::aes(x = Name, y = pAUC_10)) +
  ggplot2::geom_bar(stat = "identity") +
  ggplot2::facet_wrap(~whatfix)  + 
  ggplot2::coord_cartesian(ylim = c(min(resAllB$pAUC_10),max(resAllB$pAUC_10))) + 
  ggplot2::theme_minimal() + 
  ggplot2::theme(axis.text.x = ggplot2::element_text(angle = -90, vjust = 0.5)) +
  ggplot2::geom_hline(ggplot2::aes(yintercept = meanpAUC_10), color = "red") +
  ggplot2::xlab("") +
  ggplot2::labs(tag = "A")

p2_compare_variousLevels <- ggplot2::ggplot(res, ggplot2::aes(x = contrast, y = pAUC_10n, group = Name)) +
  ggplot2::geom_line(stat = "identity", ggplot2::aes(linetype = Name, color = Name), size = 1) + 
  ggplot2::facet_wrap(~ whatfix, scales = "free") +
  ggplot2::theme_minimal() + 
  ggplot2::theme(axis.text.x = ggplot2::element_text(angle = -90, vjust = 0.5)) +
  ggplot2::geom_hline(ggplot2::aes(yintercept = 0), color = "red") + 
  ggplot2::theme(legend.position = "bottom", legend.title = ggplot2::element_blank())

```

When comparing differential expression analysis performance, a relevant parameter is the number of proteins for which a method estimated differences (see Figure \@ref(fig:FDRfdp) Panel A), which indicates how robust the procedure works in the presence of missing observations. For each protein, we tried to determine four differences ($\Delta = (1.20, 1.25, 1.30, 1.50)$). Given 4046 proteins with at least two peptides, there are in total $16184$ possible differences. However, some methods can not estimate all of them. The set of proteins with effect size estimates might differ for each method, biasing direct comparison of scores such as pAUC. For instance, we observe that `r BiocStyle::Biocpkg("MSstats")` estimates $16058$ group differences while the mixed effect models estimates the fewest with $15940$. Hence, to conclude that one method shows a better performance, it does not suffice if the pAUC is greater, but the number of proteins with differential expression results needs to be equal or larger.

Figure \@ref(fig:FDRfdp) Panel B shows how various estimates obtained from the models, i.e., the difference between groups, $t$-statistics, and scaled $p$-values allow identifying true positives (TP) given a false positive rate (FPR) of $10%$, by displaying the partial area under the ROC. Ordering proteins by the $t$-statistic or $p$-value leads to a higher $pAUC_{10}$ than when ordering by the estimated difference among groups. Suppose an accurate estimate of the difference among groups is essential. In that case, the linear models fitted to protein intensities, calculated using Tukey's median polish, perform better than those directly modeling peptide intensities, e.g., `ropeca` or `prot_mixed_effect_moderated` (see Table \@ref(tab:describeBenchmarkedModels) Abundance column). We speculate that outliers at the peptide level do not affect the protein estimates when using Tukey's median polish method, a non-parametric method to infer protein abundances. This hypothesis could be examined, by including other forms of protein intensity inferences implemented in `prolfqua`, e.g. `top-N` or `rlm`, into the benchmark.

There are only minor differences in the $pAUC_{10}$ between the $t$-statistics or the scaled $p$-value  (see Figure \@ref(fig:FDRfdp) Panel A). Interestingly, the $pAUC$ increases when using $p$-values instead of the $t$-statistics for linear models, while it decreases for mixed effect models. The reason is an erroneous denominator degrees of freedom estimation for many proteins in the case of the mixed effect models.

We also benchmark if the $FDR$ obtained from a model is an unbiased estimate of the false discovery proportion $FDP$. Figure \@ref(fig:FDRfdp) Panel C draws on the horizontal axis the FDR determined from the model, and on the vertical axis, the FDP obtained from the confusion matrix. Most lines are below the diagonal, indicating that the FDR estimates are modestly conservative for this benchmark dataset. In the case of `r BiocStyle::Biocpkg("MSstats")`, we observe a high proportion of false discoveries for small $FDR$ values. In the case of `r BiocStyle::Biocpkg("PECA")`, the FDR estimates, obtained by Benjamini-Hochberg correcting the regulation probabilities, strongly overestimate the $FDP$.

(ref:FDRfdp) Panel A - Number of estimated contrasts for each modeling method (higher is better). Panel B - Partial area under the ROC curve at $10\%$ FPR ($pAUC_10$) for all contrasts and three different statistics: the difference among groups, the scaled $p$-value (sign(diff) $\cdot$ p.value) and the $t$-statistics (higher is better). The red line indicates the average area under the curve of all methods.  Panel C - Plots the false discovery proportion (FDP) as a function of the FDR. Ideally, the FDR should be equal to the FDP. Therefore larger distances from the diagonal are worse.

```{r FDRfdp, fig.cap = "(ref:FDRfdp)", out.width = '90%', fig.width=12, fig.height=10}
dd <- purrr::map_df(allBenchmarks, function(x){res <- x$smc$summary; res$name <- x$model_name;res})
dd <- dd |> dplyr::mutate(nrcontrasts = protein_Id * (4 - nr_missing))
dds <- dd |> dplyr::group_by(name) |> dplyr::summarize(nrcontrasts = sum(nrcontrasts))
dds$percent <- dds$nrcontrasts/max(dds$nrcontrasts) * 100

nrgg <- dds |> ggplot2::ggplot(ggplot2::aes(x = name, y = nrcontrasts )) + 
  ggplot2::geom_bar(stat = "identity", fill="white", colour = "black") + 
  ggplot2::coord_cartesian(ylim = c(min(dds$nrcontrasts) - 200, max(dds$nrcontrasts) + 10)) +
  ggplot2::theme(axis.text.x = ggplot2::element_text(angle = -90, vjust = 0.5)) +
  ggplot2::geom_text(ggplot2::aes(label = paste0(round(nrcontrasts, digits = 1), paste0("  (",round(percent, digits = 1),"%)"))),
            vjust = 0, hjust = -0.2, angle = -90) #+ 
pB <- nrgg + ggplot2::labs(tag = "B")

dd <- purrr::map_df(allBenchmarks, function(x){res <- x$get_confusion_FDRvsFDP(); res$name <- x$model_name;res})

ddb <- dplyr::filter(dd, contrast == "dilution_(4.5/3)_1.5")
ddb <- dd |> dplyr::filter(contrast == "dilution_(7.5/6)_1.25")
ddb <- dd |> dplyr::filter(contrast == "all")


pC <- ddb |> ggplot2::ggplot(ggplot2::aes(y = FDP_,  x  = scorecol )) + 
  ggplot2::geom_line(ggplot2::aes(color = model_name, linetype = model_name)) +
  ggplot2::facet_wrap(~contrast) + 
  ggplot2::geom_abline(intercept = 0, slope = 1, color = 2) + 
  ggplot2::theme(legend.position = "bottom") +
  ggplot2::labs(tag = "C")

hlay = rbind(c(2,1,1),
            c(2,3,3))

gridExtra::grid.arrange(p1_pAUC, pB, pC, layout_matrix=hlay)

```

```{r prolfquaModels}
xM <- data.frame( 
    lm = c("strategy_lm, Contrasts", "linear modelling of peptide or protein abundances and group difference estimation"),
    lmer = c("strategy_lmer, Contrasts", "mixed effect modelling of peptide or protein abundances and group differences estimation" ),
    gmi = c("ContrastsSimpleImpute" , "group difference estimation when no observations in one of the groups" ),
    ROPECA = c("ContrastsROPECA", "estimating group differences for proteins by summarizing peptide differences"),
    limma = c( "ContrastsModerated", "empirical Bayes variance shrinkage for group difference estimates (limma)"),
    pic = c("runSaint, ContrastsSaintExpress", "protein interaction scoring (SAINTExpress)"),
    prodA = c( "strategy_proDA*, Contrasts_proDA*", "adapter to the probabilistic dropout model implemented in proDA"), check.names =  FALSE)
xM <- data.frame(t(xM))

colnames(xM) <- c("prolfqua functions", "model")
knitr::kable(tibble::as_tibble(xM), caption = "Prolfqua functions to fit various models.",
             format = "latex", booktabs=TRUE) |>
    kableExtra::kable_styling(latex_options="scale_down") |> 
    kableExtra::add_footnote(label = "in development")

```


Using a benchmark dataset whose ground truth is known (see Methods), we assessed the performance of different modeling approaches implemented in `r BiocStyle::Githubpkg("fgcz/prolfqua")` (Tables \@ref(tab:prolfquaModels) and \@ref(tab:describeBenchmarkedModels)), `r BiocStyle::Biocpkg("MSstats")` and `r BiocStyle::Biocpkg("proDA")`. Table \@ref(tab:describeBenchmarkedModels) summarizes which methods we have evaluated, which MaxQuant results we used, and if the models are fitted to peptide or protein intensities. We make the R-markdown files to replicate the benchmarking available at `r BiocStyle::Githubpkg("wolski/prolfquabenchamrk")` and at [BenchmarkingIonstarData](https://wolski.github.io/prolfquabenchmark).


```{r describeBenchmarkedModels}
xMM <- data.frame(
    msstats = c("MSstats", "preprocess with default parameters",  "", "evidence.txt"),
    proDA = c("proDA", "probabilistic dropout model",  "protein", "peptide.txt"),
    imputation = c("prolfqua_impute", "ContrastsSimpleImpute, ContrastsModerated", "protein",  "peptide.txt"),
    lmmed = c("prolfqua_lm_mod", "strategy_lm, Contrasts, ContrastsModerated",  "protein", "peptide.txt"),
    prot_merged = c("prolfqua_merged", "addContrastResults( prefer = prot_med_lm_moderated , add = prot_imputation) *",  "protein", "peptide.txt"),
    mixed = c("prolfqua_mix_eff_mod", "strategy_lmer, Contrasts, ContrastsModerated", "peptide", "peptide.txt" ),
    ropeca = c("prolfqua_ropeca", "strategy_lm, Contrasts, ContrastsModerated, ContrastsROPECA", "peptide", "peptide.txt" )
)

xMM <- data.frame(t(xMM))
rownames(xMM) <- NULL
colnames(xMM) <-  c("Label","Description","Abundance","Input File")
kableExtra::kable(xMM,
             caption = paste0("All benchmarked models. ",
             "Description - prolfqua function names, ",
             "Abudances - indicates if model is fitted to peptide or protein abundances, ",
             "Input File - name of MaxQuant file used as input."),
             format = "latex", booktabs=TRUE) |>
    kableExtra::kable_styling(latex_options="scale_down") |> 
    kableExtra::add_footnote(label = "Merges results of the 'prot_med_lm_moderated' and 'prot_imputation' modeling pipeline, preferring those of 'prot_med_lm_moderated' if available.")

```

Since only technical replicates are available for each dilution, essential sources of variation present in every experiment, such as the biochemical and biological, are not measured. Therefore, the dataset captures only the variance from the chromatography, electro-spray, and mass spectrometric measurement method. Thus, while we can extrapolate some of the results obtained to more realistic datasets, we need to be careful not to over-interpret the results. Specifically, the observed variances will be higher in more realistic experiments, and the power will be lower for the same sample sizes. Furthermore, the proportion of missing observations in real-life datasets might also be higher.

We can conclude that if we want to sort the proteins according to the likelihood of being differentially regulated to perform gene set enrichment analysis [@subramanian2005gene], the $t$-statistic is better suited than the fold-change estimate. Modeling the degrees of freedom when computing the $p$-values might improve the inference. However, this improvement is minuscule (see Figure \@ref(fig:FDRfdp) panel B).
There is no such improvement for the mixed effect model, most likely because the degrees of freedom are erroneously estimated for many models. Furthermore, for the fixed effect linear model, the empirical Bayes variance shrinkage, as suggested by @Smyth2004linear, consistently improves the ranking of proteins compared with the unmoderated estimates (not shown) and fails to do so for mixed effect models.

Computing the statistics at the peptide level, e.g., the $t$-statistics or $p$-value, then summarizing these statistics using their median produces the highest $AUC$ scores among all the tested models (see Figure \@ref(fig:FDRfdp) Panel A left). Furthermore, by using the Beta distribution to model the number of peptides observed, we can further improve the $pAUC$ scores (see Figure \@ref(fig:FDRfdp) Panel A center). However, the properties of Beta-based probabilities are not well understood; for instance, the p-values are not uniformly distributed under the null hypothesis (not shown). Furthermore, the FDR estimates obtained when correcting for multiple testing with the Benjamini-Hochberg method are biased and overestimate the false discovery proportion (see Figure \@ref(fig:FDRfdp) Panel C). Therefore, we can not recommend this method if an unbiased estimate of FDR is essential, which is frequently the case. In addition, peptides are stronger affected by missing values, reducing the number of contrasts we could estimate for the dataset using this method (see Figure \@ref(fig:FDRfdp) Panel C).

The probabilistic dropout analysis implemented in the `r BiocStyle::Biocpkg("proDA")` produces inferences comparable to those of other methods (Figure \@ref(fig:FDRfdp) Panel A). Because of the robustness of the dropout model to missing observations we obtain difference estimates for all proteins and contrasts (Figure \@ref(fig:FDRfdp) Panel B). Moreover, the estimated differences are unbiased and show high diagnostic accuracy (Figure \@ref(fig:FDRfdp) Panel A). Furthermore, the performance of the scaled $p$-values or the $t$-statistics is comparable with that of the linear model with variance moderation (`prot_med_lm_moderated`). Therefore, we are planning to integrate the `r BiocStyle::Biocpkg("proDA")` package as an additional modeling option into `r BiocStyle::Githubpkg("fgcz/prolfqua")` (see Figure \@ref(fig:ContrastUML)).

The R-package `r BiocStyle::Biocpkg("proDA")` and `r BiocStyle::Biocpkg("prolfqua")` model the missing data directly, while MSstats imputes the data using an accelerated failure model. Despite imputation, `r BiocStyle::Biocpkg("MSstats")` does not estimate group differences for more proteins and does not achieve a higher $pAUC$ score than `r BiocStyle::Githubpkg("fgcz/prolfqua")`. Furthermore, Figure \@ref(fig:FDRfdp) Panel C shows that when using `r BiocStyle::Biocpkg("MSstats")` the proportion of false discoveries might be very high even when filtering for a low FDR.


We focused our benchmark on comparing the statistical modeling methods using three different scores, while we fixed the pre-processing steps. However, there are other equally or even more important parameters of a protein quantification pipeline [@frohlich2022benchmarking]. One of them is the normalization of the intensities within the samples to remove systematic differences [@pursiheimo2015optimization]. The method used to infer protein intensities from peptide intensities is an additional important factor [@Grossmann2010]. For instance, the original `r BiocStyle::Biocpkg("proDA")` publication uses MaxLFQ [@cox2014accurate] protein estimates. However, when using MaxLFQ intensities reported by MaxQuant, the $pAUC_{10}$ is significantly lower ($pAUC_{10}(\textrm{t-statistics}) = 66%$) compared with results obtained when protein abundances are estimated from peptide abundances using Tukey's median polish ($pAUC_{10}(\textrm{t-statistics}) = 73%$). Last but not least, the software [@Cox2008MaxQuant; @yu2020fast], identifying proteins and generating the quantification values can also significantly contribute to the performance of the entire pipeline, altering the number of identified proteins and the sensitivity and specificity of the differential expression analysis.  

# Conclusion

`r BiocStyle::Githubpkg("fgcz/prolfqua")` allows for considerable flexibility to model quantitative proteomics experiments. Various types of models are available (see Figure \@ref(fig:ContrastUML) and Table \@ref(tab:prolfquaModels)), and the contrast specification is explicit and consistent for all models. The modular design of `r BiocStyle::Githubpkg("fgcz/prolfqua")`, allows for adding new features, e.g., generalized linear models (_glm_'s) to model the presence or absence information of a protein, or robust linear models (_rlm_'s), in the future. R's formula interface for linear models is flexible, widely used, and well documented [@faraway2016extending]. We use the formula interface to specify the models, making it easy to reproduce an analysis performed with `r BiocStyle::Githubpkg("fgcz/prolfqua")` in other statistical methods programming languages. However, the developed framework is flexible enough to in the future integrate other modeling methods, e.g., the probabilistic dropout model [@bioxrvproDA2020] or accurate variance estimation [@zhu2020deqms]. Hence, `r BiocStyle::Githubpkg("fgcz/prolfqua")` enables you to call various methods and makes selecting the best differential expression analysis algorithm for your problem easy.

When comparing statistical modeling methods for the differential expression analysis, we assessed performance measures such as the number of estimated contrasts, the $pAUC$, and if the FDR is an unbiased estimate of the FDR. It is relevant that an analysis pipeline shows good performance in all these categories. Leveraging these computer experiments, we can provide the following advice: i) estimate protein abundances from peptide abundances using a robust or nonparametric regression method; ii) use linear models because they show good performance in all categories; iii) if the measurements are correlated, as for technical replicates, mixed effect models might work if the sample sizes are large; if not, aggregate the replicates and fit a linear model instead; iv) if you use fixed-effect linear models, apply variance moderation to improve the $t$-statistics and $p$-value estimates; v) If you want to sort your protein lists to perform gene set enrichment analysis, use the $t$-statistic instead of the difference; vi) do not impute missing observation but statistically model missingness to estimate parameters, i.e., group differences. Finally, the differential expression analysis result obtained with prolfqua are comparable to or better than when using other differential expression analysis tools.


In summary, `r BiocStyle::Githubpkg("fgcz/prolfqua")` is an easy-to-use and feature-rich R package to analyze quantitative mass spectrometric data with simple or complex experimental designs. It also can generate conclusive reports and to benchmark MS software and statistical methods. Furthermore, with minimal adaptations, this R package can analyze different quantitative proteomics data (e.g., labeling-based TMT-, PRM- DIA-data).
We provide documentation, in vignette format, at the website https://github.com/fgcz/prolfqua/. This document was created using Rmarkdown. All the code needed to replicate the document or the benchmark results is available at: https://github.com/wolski/prolfquabenchmark.

# Acknowledgements {.unlisted .unnumbered}

The authors thank the technology platform fund (TPF) of the University of Zurich and all FGCZ proteomics colleagues for fruitful discussions.


# Abbreviations {.unlisted .unnumbered}

\begin{table}
\centering
\begin{tabular}{ll}
\toprule
Abbreviations & Explaination\\
\midrule
AUC    &Area Under the Curve                               \\
CDF    &Cumulative Distribution Function                   \\
ESI-MS &Electro-Spray-Ionization Mass Spectrometry         \\
$FDP$  &False Discovery Proportion                         \\
$FDR$  &False Discovery Rate                               \\
LC     &Liquid Chromatography                              \\
LC-MS  &Liquid Chromatography followed by Mass Spectrometry\\
LOD    &Limit Of Detection                                 \\
MAR    & Missing At Random                                 \\ 
MCAR   & Missing Completely At Random                      \\
MS     &mass spectrometry                                  \\
OO     &Object-Oriented                                    \\
UML     &       Unified Modeling Language                  \\
\bottomrule
\end{tabular}
\end{table}

\newpage

# References {-}

<div id="refs"></div>

\newpage

# Appendix  {.unnumbered}

## Creating a prolfqua configuration {.unlisted .unnumbered}

The following code demonstrates how we use `r BiocStyle::Githubpkg("fgcz/prolfqua")` to analyze protein intensities reported in the MSFragger `combined_protein.tsv` file.
First, we create a tidy table containing the protein abundances by reading the `combined_protein.tsv` file using  `tidy_MSFragger_combined_protein.` Then, we read the sample annotation from the file `annotation.xlsx` file. Next, we create an `AnalysisTableAnnotation` R6 object. 
Bottom-up proteomics data is hierarchical, i.e., a protein has peptides, peptides might be modified, etc. Therefore, the `AnalysisTableAnnotation` has a `hierarchy` field storing a list with an entry for each hierarchy level.
Since `combined_portein.tsv` only holds protein level data, the hierarchy list has one element, and we use it to specify which column contains the protein identifiers. We also need to define which column contains the protein abundances we want to use for the data analysis.
Finally, we have to specify which columns contain the explanatory variables of the analysis. The `AnalysisTableAnnotation` has the field `factors,` a list with as many entries as explanatory variables. Here we include two explanatory variables, the dilution, specified in the column 'sample', and 'run' stored in the column 'run_ID', representing the order of the measurement.


```{r echo=TRUE}
datadir <- file.path(find.package("prolfquadata") , "quantdata")
inputFragfile <-  file.path(datadir, "MSFragger_IonStar2018_PXD003881.zip")
inputAnnotation <- file.path(datadir, "annotation_Ionstar2018_PXD003881.xlsx")
# read input annotation
annotation <- readxl::read_xlsx(inputAnnotation)

protein <- tibble::as_tibble(
    read.csv(unz(inputFragfile,"IonstarWithMSFragger/combined_protein.tsv"),
             header = TRUE, sep = "\t", stringsAsFactors = FALSE))

# read combined_protein.tsv 
protein <- prolfqua::tidy_MSFragger_combined_protein(protein)
# remove proteins identified by a single peptide
protein <- protein |> dplyr::filter(unique.stripped.peptides > 1)

# annotate the data
merged <- dplyr::inner_join(annotation, protein)
atable <- prolfqua::AnalysisTableAnnotation$new()
atable$fileName = "raw.file"
# specify column containing protein identifiers
atable$hierarchy[["protein_Id"]] = "protein"

# column with protein abundances
atable$setWorkIntensity("total.intensity")

# the factors of the analysis
atable$factors[["dilution."]] = "sample"
atable$factors[["run"]] = "run_ID"

config <- prolfqua::AnalysisConfiguration$new(atable)

adata <- prolfqua::setup_analysis(merged, config)
lfqdata <- prolfqua::LFQData$new(adata, config)
# remove small intensities
lfqdata$remove_small_intensities()

```

## Session information {.unlisted .unnumbered}

```{r sessioninfo, echo=FALSE}
sessionInfo()
```

## Miscellaneous {.unlisted .unnumbered}


(ref:tabCompletion) The screenshot displays the command-line completion (tab completion) of RStudio on the `prolfqua::LFQData` R6 object. In the example, it shows the getter methods of the object.

```{r tabCompletion, echo=FALSE, fig.cap="(ref:tabCompletion)", out.width = '66%'}
knitr::include_graphics("codeSnippet1TabCompletion.png")
```


```{r proLFQuaSticker, echo=TRUE, out.height="5cm", eval=TRUE, fig.cap="Sticker maintainer: Witold E. Wolski; License: Creative Commons Attribution CC-BY. Feel free to share and adapt, but don't forget to credit the author."}

file.path("hexStickerProlfqua.png") |>
  knitr::include_graphics()
```

