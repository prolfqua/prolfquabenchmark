---
title: prolfqua&colon; A Comprehensive R-package for Proteomics Differential Expression Analysis
author: 
  - Witold E. Wolski $^{\dagger\ddagger}$^[[Functional Genomics Center Zurich (FGCZ)](https://fgcz.ch) - [University of Zurich](https://uzh.ch)/[ETH Zurich](https://ethz.ch), Winterthurerstrasse 190, CH-8057 Zurich, Switzerland. $^\ddagger$[Swiss Institute of Bioinformatics (SIB), Quartier Sorge - Batiment Amphipole, 1015 Lausanne, Switzerland](https://www.sib.swiss/) $^\dagger$Correspondence&colon; \texttt{wew@fgcz.ethz.ch}]
  - Paolo Nanni$^{\ast}$
  - Jonas Grossmann$^{\ast\ddagger}$
  - Maria d'Errico$^{\ast\ddagger}$
  - Ralph Schlapbach$^{\ast}$
  - Christian Panse$^{\ast\ddagger}$
output: 
  bookdown::pdf_document2:
      toc: true
      toc_appendix: false
      toc_bib: false
      keep_tex: true
      citation_package: natbib
date: "`r format(Sys.time())`"
bibliography: prolfqua.bib
vignette: >
  %\VignetteIndexEntry{prolfqua&colon; A Comprehensive R-package for Proteomics Differential Expression Analysis} 
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
abstract: |
    Mass spectrometry is widely used for quantitative proteomics studies, relative protein quantification, and differential expression analysis of proteins. There is a large variety of quantification software and analysis tools. Nevertheless, there is a need for a modular, easy-to-use application programming interface in *R* that transparently supports a variety of well principled statistical procedures to make applying them to proteomics data, comparing and understanding their differences easily. The `r BiocStyle::Githubpkg("fgcz/prolfqua")` package integrates essential steps of the mass spectrometry-based differential expression analysis workflow: quality control, data normalization, protein aggregation, statistical modeling, hypothesis testing, and sample size estimation. The package makes integrating new data formats easy. It can be used to model simple experimental designs with a single explanatory variable and complex experiments with multiple factors and hypothesis testing. The implemented methods allow sensitive and specific differential expression analysis. Furthermore, the package implements benchmark functionality that can help to compare data acquisition, data preprocessing, or data modeling methods using a gold standard dataset. The application programmer interface of `r BiocStyle::Githubpkg("fgcz/prolfqua")`  strives to be clear,  predictable, discoverable, and consistent to make proteomics data analysis application development easy and exciting. Finally, the `r BiocStyle::Githubpkg("fgcz/prolfqua")` *R* package is available on GitHub https://github.com/fgcz/prolfqua, distributed under the MIT license. It runs on all platforms supported by the [*R* free software environment for statistical computing and graphics](https://www.r-project.org/).

urlcolor: blue
---


```{r setup, include=FALSE}

Biocpkg <- function(pkg, label = NULL){
  url <- file.path("https://bioconductor.org/packages", pkg)
  BiocStyle:::labelled_link(pkg , label , url)
}

## See ACS guidelines [https://publish.acs.org/publish/author_guidelines?coden=ancham#manuscript_text_components]
knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE,
                      fig.width = 7,
                      fig.height = 5)
ggplot2::theme_set(ggplot2::theme_classic())
library(BiocStyle)
options(width = 70)
```

\newpage

# Introduction

Proteins carry out most crucial functions and give structure to living cells. Hence, measuring changes in protein abundance is the subject of active research [@vidova2017review]. Bottom-up mass spectrometric methods, where proteins are specifically and reproducibly digested into protein fragments - peptides, are employed to identify and quantify proteins in complex samples containing hundreds to thousand of proteins [@Bubis2017; @da2020philosopher]. The peptides are first separated by their chemical and physical properties using liquid chromatography (LC). Afterward, they are ionised, weighed, identified, and quantified using mass spectrometric techniques, e.g., electro-spray-ionization mass spectrometry (ESI-MS). Finally, peptide identification is achieved by fragmenting and matching the measured fragment masses to theoretical masses computed from known peptide sequences [@eng2015deeper; @yu2016pipi; @kong2017msfragger]. For quantification, intact peptide ions [@yu2020fast; @Cox2008MaxQuant] or products of peptide ion fragmentation [@rost2014openswath; @demichev2020dia] are counted and aggregated to obtain peptide abundances. Finally, the identified and quantified peptides are assigned to proteins based on protein sequence information [@nesvizhskii2005interpretation].

Proteomics quantification experiments enable monitoring relative abundances of thousands of proteins in biological samples. Most studies use parallel-group designs, where one or many treatment groups are compared to the control group [@de2022apoe2; @laubscher2021baf]. More recently, more complex experimental designs with an increasing number of samples are studied, e.g., factorial designs and longitudinal studies (time series), sometimes with repeated measurements on the same subject [@tan2022proteomic; @meier2021reduced]. The data can be modeled using linear fixed-, mixed-, or random-effects models [@Bates2015FittingJSS]. Based on these models, tests can be applied to examine whether specific factors and factor interactions are significant, e.g., it can be tested if differences in protein abundance between groups are statistically significant. 

An important aspect of mass spectrometric data are missing peptide and protein quantifications. @rubin1976inference classified missing data problems into three categories: missing completely at random (MCAR), missing at random (MAR), and missing not at random (MNAR). For instance, in data-dependent acquisition (DDA) MS, only a limited number of MS1 signals are selected for fragmentation and identified. Peptide quantification algorithms transfer identification information between MS1 features in different samples by aligning the data using retention time and mass information, thus reducing the amount of missing data. Nevertheless, highly abundant proteins can suppress the detection of other proteins in a sample, a MAR mechanism. Furthermore, a weak correlation between the number of missing measurements in a group and the abundance of the quantified protein can be observed, which is caused by the limit of detection (LOD), a MNAR mechanism [@mcgurk2020use].

Several data analysis packages exist to model mass spectrometry protein quantification experiments, e.g., `r Biocpkg("limma")` [@Ritchie2015], `r Biocpkg("MSstats")` [@Choi2014], `r Biocpkg("PECA")` [@Suomi2017bEnhanced],  `r Biocpkg("msqrob2")` [@Goeminne2016] or `r Biocpkg("proDA")` [@bioxrvproDA2020], to mention some, all implemented in *R* [@RCoreTeam2021]. Each of them has some unique features, for example, *MSstats* determines the statistical model from the structure of the sample annotation, which allows users with limited statistical knowledge to perform differential expression analysis (DEA). At the same time, *limma* allows the specification of a design matrix using a linear model formula and implements the empirical Bayes variance shrinkage method. The package *PECA* performs a roll-up of peptide level differences and peptide level $p$-value estimates obtained from *limma* or *PECA*, to protein level estimates. Furthermore, *msqrob2* combines two models: robust linear models fitted to protein abundances and a quasibinomial generalized linear model fitted to peptide counts, into Hurdle model. The *proDA* package implements a linear probabilistic dropout model to account for missing data without imputation.

Of note are the various approaches to handle missing observations, which are inherent in mass spectrometric bottom-up experiments. For instance, *MSstats* handles missing data by feature filtering and imputation. Other means of modeling missing observations are the Hurdle models discussed by @goeminne2020msqrob, while the *R* package *proDA* models missingness using probabilistic dropout models [@bioxrvproDA2020].

We can use all the *R* packages discussed when analyzing parallel-group designs using a single explanatory variable and contrasting groups. However, we can use only some of them if we want to model factorial designs or repeated measurements. Table \@ref(tab:tableOverview) gives an overview of the models and features supported by these packages. We see that, for instance, *limma* and *proDA* allow us to fit a comprehensive variety of models and test various hypotheses; however, good knowledge of the design matrix specification using the *R* formula interface is required [@law2020guide].


```{r tableOverview}
caption = "Rows - R packages, Columns - types of models supported: pd - parallel design, rm - repeated measurements, fd - factorial design, int - interactions among factors, mem - mixed effect models, eb - empirical Bayes, md - missing data modelling. Y - yes, * - repeated measurements are modeled using a fixed effect."

xx <- list(
  MSstats = c("pd" = "Y", "rm" = "Y", "mem" = "Y"),
  PECA = c("pd" = "Y", "rm" = "Y", eb = "Y"),
  limma  = c("pd" = "Y", "rm" = "Y", fd = "Y", int = "Y",eb = "Y"),
  msqrob2 = c("pd" = "Y", "rm" = "Y", fd = "Y", int = "Y", mem = "Y", eb = "Y", md = "Y"),
  proDA = c("pd" = "Y", "rm" = "Y", fd = "Y", int = "Y", eb = "Y", md = "Y"),
  prolfqua = c("pd" = "Y", "rm" = "Y", fd = "Y", int = "Y", mem = "Y",eb = "Y", md = "Y"))

bb <- data.frame(dplyr::bind_rows(xx))
rownames(bb) <- names(xx)
knitr::kable(data.frame(bb), format = "latex", caption = caption)
```

When developing the *R* package `r BiocStyle::Githubpkg("fgcz/prolfqua")` we were inspired by the *R* package `r BiocStyle::CRANpkg("caret")` [@caret2008] which enables us to call various machine learning (ML) methods, and makes selecting the best ML algorithm for your problem easy. We aimed for a similar *R* package for the DEA of quantitative proteomics data. However, the existing packages differ widely regarding supported designs, model specifications, and output formats. At the same time, they have the following features in common: fitting linear models to either peptide or protein intensities, determining differences among groups, and afterward applying empirical Bayes variance shrinkage. Therefore, the revised goal was to provide a modular object-oriented design, with *R* linear models as a core, and add features such as $p$-value aggregation, variance shrinkage, or modeling of missing observations.

Furthermore, `r BiocStyle::Githubpkg("fgcz/prolfqua")` also includes methods specific to proteomics data. For example, we implemented strategies to estimate protein intensities from peptide intensities: top N [@Grossmann2010], Tukey's median polish  [@tukey1977exploratory], robust linear models [@goeminne2020msqrob]. Peptide or protein abundances can then be scaled and transformed using robust scaling, _quantile_ normalization or `r Biocpkg("vsn")` to remove systematic differences among samples and heteroscedasticity. In this respect, *prolfqua* can be compared with *R* packages such as `r Biocpkg("DEP")` [@DEP2018] or `r Biocpkg("POMA")` [@POMA2021] which support the entire 
DEA pipeline.

Since group sizes are relatively small, typically with four or five subjects per group, the high power of the tests is a relevant criterion to assess the modeling method. The quantified proteins can be ranked using the estimated fold-change, $t$-statistics, or scaled $p$-value, and afterward subjected to gene set enrichment (GSEA) or over-representation analysis [@subramanian2005gene] to determine up or down-regulated groups of proteins. Furthermore, the statistical model must provide an unbiased estimate of the false discovery rate (FDR) to manage expectations when selecting protein lists for follow-up experiments. We will use the partial area under the receiver operator curve (ROC) to assess the power of the tests and compare the FDR with the false discovery proportion ($FDP$). We use the *IonStar*[@shen2018ionstar] and *CPTAC*[@edwards2015cptac]  datasets, processed with *MaxQuant* and *FragPipe* to benchmark the modeling methods implemented in *prolfqua* and to compare our results with those of *MSstats*, *msqrob2* and *proDA*. Although other benchmark datasets exist[@wessels2012comprehensive;@o2018proteome], the *IonStar* dataset has the advantage that the expected differences, for the spike in proteins, among groups are small compared to other benchmark datasets, making DEA more difficult and enabling us to see performance differences among the modeling methods.

\newpage
# Methods

## Implementation

We store all the data needed for analysis in a data frame as tidy data, i.e., every column is a variable, every row is an observation, and every cell is a single value [@tidydata2014]. Using an R6 [@R6cite] configuration object (Figure \@ref(fig:LFQData)), we specify what variable is in which column making it easy to integrate new inputs in *prolfqua* if provided as a tidy data. For example, to visualize tidy Spectronaut[@bruderer2015extending], DiaNN[@demichev2020dia], or Skyline[@maclean2010skyline] outputs, or data in *MSstats*[@Choi2014] format, only a few lines of code are needed to update the *prolfqua* `AnalysisTableConfiguration` configuration. The configuration encapsulates the differences among the various input formats in column names and enables the use of the methods without additional arguments. An example code for creating an *FragPipe*[@yu2020fast] configuration can be found in the Supplementary Section "Creating a prolfqua configuration". We implemented methods that transform the data into tidy data for popular software like *MaxQuant*[@Cox2008MaxQuant], or *FragPipe*, which stores the same variable, e.g., intensity, in multiple columns, one for each sample. Relying on the tidy data table enables us to interface with many data manipulation, visualization, and modeling methods, implemented in base *R* [@RCoreTeam2021] and the tidyverse [@tidyverse2019], easily. We use R6 classes to structure the functionality of the package (see Figure \@ref(fig:LFQData) and Figure \@ref(fig:ContrastUML)). R6 classes are well supported by command-line completion features (see Supplementary Figure 8) in *RStudio* [@RStudio2022], and help to implement argument free functions.

(ref:LFQData) Class Diagram of classes representing the proteomics data. The `LFQData` class encapsulates the quantitative proteomics data stored in a table of tidy data. An instance of the AnalysisTableConfiguration class specifies a mapping of table columns to sample names, peptide or protein identifiers, explanatory variables, and response variables. The `LFQDataPlotter` class and other classes decorate the `LFQData` class with additional functionality. For instance, the `LFQDataStats` and `LFQDataSummary` reference the `LFQData` class and group methods for variance and sample size estimation or summarizing peptide and protein counts. Furthermore, the `LFQDataTransformer` and `LFQDataAggregator` classes group functions for data normalization and for estimating protein from peptide intensities.

```{r LFQData, echo=FALSE, fig.cap="(ref:LFQData)", out.width = '90%'}
knitr::include_graphics("LFQData_UML_LARGE.png")
```

*R*'s formula interface for linear models is flexible, widely used, and well documented [@faraway2016extending; @law2020guide]. We use the formula interface to specify the models, making it easy to reproduce an analysis performed with *prolfqua* in other statistical programming languages. In addition, we implement features specific to high throughput experiments, such as the empirical Bayes variance and $p$-value moderation, which utilizes the parallel structure of the protein measurements and the analysis [@Ritchie2015]. We also compute probabilities of differential protein regulation based on peptide level models [@Suomi2017bEnhanced]. We used R6 classes to encapsulate the statistical modelling functionality in *prolfqua* (see Figure \@ref(fig:ContrastUML)). We specify a contrast interface (`ContrastsInterface`). Several implementations allow to perform DEA given linear or mixed effect models (`Contrasts`), variance shrinkage (`ContrastsModerated`), or to impute contrasts in cases when observations are missing for an entire group of samples (`ContrastsSimpleImpute`). Further implementations of the interface encapsulate and integrate DEA results of external tools such as *proDA* or of *SAINTexpress* [@teo2014saintexpress] used to analyze data from protein interaction studies.

(ref:ContrastUML)  Unified modeling language (UML) diagram of modeling and contrast related classes. Different strategies, e.g., _lm_, _lmer_, and _glm_ (Table \@ref(tab:prolfquaModels)), reference methods to fit models, and compute contrasts. The model builder method fits the statistical model given the data and a strategy. The models are used to analyze variance (ANOVA) or to estimate contrasts. All classes estimating contrasts implement the _ContrastsInterface_. Results of external tools, e.g., *SAINTexpress*, or *proDA* are adapted to implement the Contrasts Interface.


```{r ContrastUML, echo=FALSE, fig.cap="(ref:ContrastUML)", out.width = '90%'}
#fig_svg <- cowplot::ggdraw() + cowplot::draw_image("ContrastClassesUML.svn")
#plot(fig_svg)

knitr::include_graphics("ContrastClassesUML_LARGE.png")
```     

```{r prolfquaModels}
xM <- data.frame( 
    lm = c("strategy_lm, Contrasts", "linear modelling of peptide or protein abundances and group difference estimation"),
    lmer = c("strategy_lmer, Contrasts", "mixed effect modelling of peptide or protein abundances and group differences estimation" ),
    gmi = c("ContrastsSimpleImpute" , "group difference estimation when no observations in one of the groups" ),
    ROPECA = c("ContrastsROPECA", "estimating group differences for proteins by summarizing peptide differences"),
    limma = c( "ContrastsModerated", "empirical Bayes variance shrinkage for group difference estimates (limma)"),
    pic = c("runSaint, ContrastsSaintExpress", "protein interaction scoring (SAINTExpress)"),
    proDA = c( "strategy_proDA*, ContrastsProDA", "adapter to the probabilistic dropout model implemented in proDA"), check.names =  FALSE)
xM <- data.frame(t(xM))

colnames(xM) <- c("prolfqua functions", "model")
knitr::kable(tibble::as_tibble(xM), caption = "prolfqua functions that can be used to fit various models.",
             format = "latex", booktabs = TRUE) |>
    kableExtra::kable_styling(latex_options = "scale_down") |> 
    kableExtra::add_footnote(label = "in development")

```

## Datasets for benchmarking

### IonStar
 

To evaluate the performance of DEA, we use the *IonStar* benchmark dataset[@shen2018ionstar], available from the Proteomics Identifications Database (PRIDE) identifier PXD003881. $DH5\alpha$ _E. coli_ lysate was spiked in human pancreatic cancer cells (Panc-1) lysate at five different levels: $3\%$, $4.5\%$, $6\%$, $7.5\%$ and $9\%$ _E. coli_. We annotated these dilutions from smallest to largest with the letters _a_, _b_, _c_, _d_ and _e_. By comparing the various dilutions, we obtain different effect sizes, e.g., when comparing dilution _e_ ($9\%$) against dilution _d_ ($7.5\%$), the expected difference is $1.2$ for E. coli proteins and $1$ for human proteins. There are four technical replicates for each dilution, hence 20 raw files in total. To compare the performance of the various methods implemented in *prolfqua* we use only the contrasts resulting in minor differences $\Delta = (1.20, 1.25, 1.30, 1.50)$, because for bigger differences all models perform similarly.


### IonStar/MaxQuant

We processed the raw data of the *IonStar* dataset using *MaxQuant* [@Cox2008MaxQuant] version Version 1.6.10.43, with *MaxQuant* default settings for orbitrap data. The text files generated by *MaxQuant* are available in the *prolfquadata* *R* package [@prolfquadata]. *MaxQuant* produces various output files which can be used for DEA. We are using the quantification results reported in the 'peptide.txt' file for DEA. However, *MSstats* is using the 'evidence.txt' file for the DEA.

### IonStar/FragPipe

We processed the raw data of the *IonStar* dataset using the *FragPipe* [@yu2020fast] Version 14, with the default workflow for label free quantification with match between runs. The text files generated by *FragPipe* are available in the *prolfquadata* *R* package [@prolfquadata].
Similarily to *MaxQuant*, the *FragPipe* software produces various outputs which can be used for DEA. We used the total protein intensities reported in the 'combined_protein.tsv' file as input for the DEA and call this dataset IonStar/FragPipe/combined_protein.tsv. Alternatively, we benchmarked the DEA using the 'MSstats.tsv' file as input and call this dataset IonStar/FragPipe/combined_protein.tsv.

### CPTAC/MaxQuant

We used the CPTAC dataset, available in the *R* package *msdata*, and described in [@Goeminne2016]. In brief, the Sigma Universal Protein Standard mixture 1 (UPS1) containing $48$ different human proteins was spiked in a protein background of 60 ng/$\mu L$ Saccharomyces cerevisiae strain BY4741. Two different spike-in concentrations were used, 6A ($0.25$ fmol UPS1 proteins/$\mu L$) and 6B ($0.74$ fmol UPS1 proteins/$\mu L$). Three replicates are available for each concentrations. The data were searched with *MaxQuant* version `1.5.2.8.`. 


## Data preprocessing for model comparison

The peptide abundances (from the *MaxQuant* _peptide.txt_ file) were $log_2$ transformed and subsequently scaled, where median and the mean absolute deviation were obtained from the human proteins only. We removed _one hit wonders_, i.e., proteins with a single peptide assignment. Protein abundances are inferred from the peptide intensities using Tukey's median polish. Finally, we fitted the fixed effect linear models, the dropout model *proDA* to protein abundances, the mixed effect linear model, the *ROPECA* model, and hurdle model implemented in *msqrob2* to peptide intensities.

## Benchmark metrics

The *IonStar* dataset contains _H. sapiens_ proteins with constant concentrations and _E. coli_ proteins with varying concentrations. We know that for _H. sapiens_ proteins, the difference $\beta$ between two dilutions should be $\beta = 0$, while for _E. coli_ proteins, we know that the difference between dilutions should be $\beta \ne 0$.

We can use various statistics to examine the alternative hypothesis $\beta \ne 0$: the contrast estimate, i.e. the $\log_2$ fold-change $\beta$, the $t$-statistic $\frac{\beta}{\sqrt{var(\beta)}}$, or the $p$-value and moderated $p$-value. For each statistic and each value of the statistics we then compute a confusion matrix (see Table \@ref(tab:knitrConfusionMatrix)). From the confusion matrix we obtain measures such as true positive rate ($TPR$), false positive rate ($FPR$), or false discovery proportion ($FDP$) which are given by:

```{r knitrConfusionMatrix}
table <- data.frame( c("beta != 0", "beta == 0", "Total"),
                     matrix(c("TP", "FP", "R", "FN", "TN", "", "P", "N", "m"),
                            ncol = 3, byrow = T))
colnames(table) <- c("Prediction \\ Truth","E.coli", "H.sapiens", "Total")

knitr::kable(table, caption = "Confusion matrix, TP - true positive, FP - false positive, FN - false negative, TN - true negatives, P - all positive cases (all $E.~coli$ proteins), N - all negative cases (all $H.~sapiens$ proteins), m - all proteins.", format = "latex" )
```

with

\begin{eqnarray}
TPR &= \frac{TP}{TP+FN} = \frac{TP}{P}\\
FPR &= \frac{FP}{FP+TN} = \frac{FP}{N}\\
FDP &= \frac{FP}{TP + FP} = \frac{FP}{R}
\end{eqnarray}

In order to compute the confusion matrices based on the $p$-value we first need to rescale it (see Equation \@ref(eq:scalepvalue)).

By plotting the $TPR$ versus the $FPR$ we obtain the receiver operator characteristic curve (ROC curve) [@pROC2011]. The area under the curve ($AUC$) or partial areas under the curve ($pAUC$), at various values of the $FPR$, are measures of performance derived from the ROC curve. Using these measures, we can compare the performances of the statistics for a single model or the various models and test if the differences are statistically significant, using a test to compare ROC curves.


## Modelling

### Robust scaling of the data

@valikangas2016systematic discuss and benchmark various methods of peptide or protein intensity normalization, such as variance stabilizing normalization [@huber2002variance] or quantile normalization [@bolstad2003comparison]. In this work, we use a robust version of the z-score, where instead of the mean we use the median $\tilde{x}$ , and instead of the standard deviation we use the median absolute deviation $\tilde{S}$:

\begin{eqnarray}
z = \frac{x - \tilde{x}}{\tilde{S}}
\end{eqnarray}

Because we need to estimate the differences among groups on the original scale, we must multiply the $z$-score by the average standard deviation of all the $n$ samples in the experiment.

\begin{eqnarray} 
z' = z \cdot \frac{1}{n}\sum_{i=1}^n \tilde{S}_i
\end{eqnarray}

To apply this transformation, we need to estimate two parameters per sample, therefore it works for experiments with thousands of proteins and experiments where only a few hundred proteins per sample are measured.
For the Ionstar dataset, we used the intensities of _H. sapiens_ proteins, whose concentrations do not change, to determine $\tilde{x}$ and $\tilde{S}$ and then applied it to all the intensities (including _E. coli_) in the sample.


### Estimating differences between groups

Given a linear model $y = \beta X$, we can compute the difference $\beta_{c}$ between two groups by the dot product of weights $c$ and model parameters $\beta$, where $c$ is a column vector with as many elements as there are coefficients $\beta$ in the linear model. If $c$ has $0$ for one or more of its rows, then the corresponding coefficient in $\beta$ is not involved in determining the contrast [@ph525xseries].

The difference estimate $\beta_c$, is given by the dot product:

\begin{eqnarray} 
\hat{\beta_{c}} &=& c^T \beta 
\end{eqnarray} 

and the variance of $\beta_c$ by:

\begin{eqnarray} 
\textrm{var} (\hat\beta_c) &=& \sqrt{ c^T \sigma^2 (X^T X)^{-1} c }
\end{eqnarray}

with $X$ being the design matrix. The degrees of freedom for the contrast are equal to the residual degrees of freedom of the linear model. For estimating contrasts from mixed effects models we used the function `contest` implemented in the *R* package `r BiocStyle::CRANpkg("lmerTest")`  and used the Satterthwaite [@Kuznetsova2017lmerTest] method to estimate the denominator degrees of freedom. These methods are available in the class `Contrast` (see Figure \@ref(fig:ContrastUML))

The package *prolfqua* provides functions to determine the vector of $parameter$ weights $c$, from a linear model and a contrast specification string. In the Supplement we provided an example, how to specify contrasts for a dataset with two explanatory variables and an interaction term (Supplementary Section "Specifying Contrasts for Models with two Factors and Interaction Term").

### Contrast estimation in presence of missing data.

Missing observations lead to different group sizes, which results in unbalanced designs. Linear and mixed effect models can handle unbalanced designs. As long as at least one observation in a group is available, and sufficient observations to estimate the variance are available, they will produce unbiased estimates. Therefore, no imputation is needed.

However, if there is no observation in a group the model fit fails. For example, suppose a protein is unobserved in all the samples of a group. In that case, a plausible explanation is that the protein abundance is below the limit of detection (LOD) of the MS instrument. In such a case, we will substitute the group mean using the expected protein abundance $A$ at the LOD $A_{LOD}$.
To estimate $A_{LOD}$ we are using the protein abundances of those groups where the protein was observed in only a single sample (see Supplementary Section "Estimating $A_{LOD}$"). Typically there are many such cases, and hence we take the median.

When computing differences $\Delta$ among two groups $a$ and $b$, we will use either the group mean $\bar{a}$ or $\bar{b}$ estimated from the data. However, if for instance no observations are present in group $b$, we will use $\bar{b} = A_{LOD}$. Furthermore, if $\bar{a} < A_{LOD}$ we also set $\bar{a} = A_{LOD}$, or more formally:

$$
\Delta = 
\begin{cases}
\bar{a} - A_{LOD} & \text{if} ~~ \bar{a} > A_{LOD}\\
0  &\text{if} ~~ \bar{a} < A_{LOD}.
\end{cases}
$$
We use the pooled variance in all groups to estimate the protein variance, assuming they are the same. The pooled variance $s_p^2$ is given by:

\begin{equation} 
s_p^2=\frac{\sum_{i=1}^k (n_i - 1)s_i^2}{\sum_{i=1}^k (n_i - 1)}
\end{equation} 

with $n_i$ the number of observations, and $s_i$ the standard deviation in each group.
The standard deviation for the $t$-statistics is then given by:

\begin{equation} 
s = \sqrt{\frac{2n_g s_p^2}{n}},
\end{equation} 

Where $n_g$ is the number of groups and $n$ is the number of observations.
If variance can not be estimated for a protein, because there are too few observations in other groups, we use the median pooled variance of all other proteins in the dataset. This method is implemented in the class `ContrastSimpleImpute` (see Figure \@ref(fig:ContrastUML)).

### $p$-value moderation

From the linear and the mixed effect models, we can obtain the residual standard deviation $\sigma$, and degrees of freedom $df$. @Smyth2004linear discuss how, to use the $\sigma$ and $df$ of all models to estimate the corresponding priors and posterior $\tilde \sigma$. These can be used to moderate the $t$-statistics by:

\begin{equation} 
\tilde{t}_{pj} = \frac{t_{pj} s_p}{\tilde{s}_p}.
\end{equation} 

We implemented this method in the class `ContrastModerated` (Figure \@ref(fig:ContrastUML)).

### Summarizing peptide level differences and p-values on protein level

To summarize peptide level models to protein models, we apply the method suggested by @Suomi2017bEnhanced that uses the median scaled $p$-value of the peptide models and the cumulative distribution function of the Beta distribution (CDF) to determine a regulation probability of the protein.

To obtain the $\tilde{p}$ of a protein we first rescaled the peptide $p$-values by taking the sign of the fold-change $\hat \beta$ into account, i.e.:

\begin{equation}
p_{s} =
  \begin{cases}
1-p, & \textrm{if}~ \hat{\beta} > 0\\
p-1, & \textrm{otherwise}
\end{cases}
(\#eq:scalepvalue)
\end{equation}

Afterwards, the median scaled $p$-value $\tilde{p}_s$ is determined and, using the transformation below, transformed back onto the original scale:

\begin{equation}
\tilde{p} = 1 - |\tilde{p}_{s}|
\end{equation}

Because we use the median with the i-th order statistic $i = \frac{n}{2} + 0.5$, we parametrize the CDF of the Beta distribution with $\gamma = i = \frac{n}{2} + 0.5$ and $\delta = n - i + 1 = n - (\frac{n}{2} + 0.5) + 1 = \frac{n}{2} + 0.5 = \gamma$. We implemented this method in the class `ContrastROPECA` (Figure \@ref(fig:ContrastUML)).


# Results and Discussion


## Example analysis workflow

The code snippets in this section demonstrate how a DEA workflow can be implemented using the *prolfqua* *R* package. To speed up the computation of these examples, we use a subset of the Ionstar dataset generated by randomly selecting $400$ proteins. First, we remove all proteins with a single peptide and all observations for which *MaxQuant* reports zero intensities which leaves $332$ proteins. Next, peptide abundances are $\log_2$ transformed and robust z-score scaled using the method `robscale`. Then, using the `LFQDataPlotter` class, we show the distribution of the normalized peptide abundances in Figure \@ref(fig:figure1) Panel A. Afterwards, protein intensities are estimated from peptide intensities using Tukey's median polish. Figure \@ref(fig:figure1) Panel B shows the peptide intensities and the estimated protein intensities. Next, we compute the standard deviation of all the proteins in each group and display their distribution using violin plots (Panel C). Finally, we create a boxplot (Panel D) showing the abundance of one protein.

```{r figure1,  echo = TRUE , fig.cap="(ref:scaling)", out.width = '90%', fig.width = 8, fig.height = 8}
R.version.string; packageVersion("prolfqua")

## read MQ peptide.txt and annotation table
startdata <- prolfqua::tidyMQ_Peptides(system.file(
  "samples/maxquant_txt/tiny2.zip", package = "prolfqua"))
annot <- readxl::read_xlsx(system.file(
  "samples/maxquant_txt/annotation_Ionstar2018_PXD003881.xlsx", package = "prolfqua"))
startdata <- dplyr::inner_join(annot, startdata, by = "raw.file")

## create MaxQuant configuration
config <- prolfqua::create_config_MQ_peptide()

## specify explanatory variable
config$table$factors[['dilution.']] = "sample"

## create R6 object
lfqpep <- prolfqua::LFQData$new(startdata, config, setup = TRUE) 

## remove observation with 0 intensity and filter for 2 peptides per protein
lfqpep$remove_small_intensities()$filter_proteins_by_peptide_count()

## transform intensities
lfqpep <- lfqpep$get_Transformer()$log2()$robscale()$lfq
lfqpep$rename_response("log_peptide_abundance")
agr <- lfqpep$get_Aggregator()
lfqpro <- agr$medpolish()
lfqpro$rename_response("log_protein_abundance")

## plot Figure 3 panels A-D
pl <- lfqpep$get_Plotter()
panelA <- pl$intensity_distribution_density() +
  ggplot2::labs(tag = "A") + ggplot2::theme(legend.position = "none")
panelB <- agr$plot()$plots[[54]] + ggplot2::labs(tag = "B")
panelC <- lfqpro$get_Stats()$violin() + ggplot2::labs(tag = "C")
pl <- lfqpro$get_Plotter()
panelD <- pl$boxplots()$boxplot[[54]] + ggplot2::labs(tag = "D")
ggpubr::ggarrange(panelA, panelB, panelC, panelD)
```

(ref:scaling) Panel A - Density plot of peptide intensity distributions for $20$ samples. For each sample a line with a different colour is shown. Panel B - Peptide intensities for protein HFQ_ECOLI are shown using a line of different colour, and the protein intensity estimate is shown using a fat black line, Panel C - distribution of standard deviations of all proteins in each dilution group ($a$, $b$, $c$, $d$, $e$) and overall (all), Panel D - Distribution of protein intensities of Protein HFQ_ECOLI in each dilution group.

The following code example illustrates how we compute differences among groups. First, the linear model and the differences are specified. Afterward, the model is fitted to the data using the `build_model` function. Next, we estimate the contrasts either from the linear model using the `Contrasts` class or directly from the data using the `ContrastsSimpleImpute` class. Afterward, we apply $t$-statistic moderation using the `ContrastModerated` class. Finally, the `merge_contrasts_results` function merges both sets of contrast estimates, preferring the one obtained from the linear model if both are available. Then we create the plots shown in Figure \@ref(fig:figure2). Panel A shows the distribution of the $p$-values, Panel B shows the volcano plot for each comparison, and Panel C shows a Bland-Altman plot reporting the difference between groups as a function of the rank of the protein abundance.

(ref:figure2) Panel A - Histogram showing the distribution of p-values for $332$ proteins for contrasts "e_vs_d" and "d_vs_c". Panel B -  Volcano plot showing $-\log_{10}$ transformed $FDR$ as a function of the difference between groups for $332$ proteins. With black dots, we show effect size and FDR estimates obtained from the linear model, while in green, we plot those obtained using imputation. Finally, panel C shows the difference between groups, as a function of the rank of the abundance of the proteins.



```{r figure2, fig.cap="(ref:figure2)", echo=TRUE, out.width = '90%', fig.width = 8, fig.height = 5}
## specify differences among groups
contrasts <- c(
  "e_vs_d" =   "dilution.e - dilution.d",
  "d_vs_c" =   "dilution.d - dilution.c"
)
## fit model
lmmodel <- paste(lfqpro$response()," ~ dilution.")
modelFunction <- prolfqua::strategy_lm( lmmodel, model_name = "lm")
models <- prolfqua::build_model(lfqpro, modelFunction)

## compute contrasts from linear model
contr <- prolfqua::Contrasts$new(models, contrasts, modelName = "lm")

## compute contrasts using imputation imputation
conI <- prolfqua::ContrastsSimpleImpute$new( lfqpro, contrasts, modelName = "imp")

## merge contrasts, to obtain differences for all proteins
contrasts <- prolfqua::merge_contrasts_results(prefer = contr, add = conI)

## moderate t-statistics and p-values
prolfqua::ContrastsModerated$undebug("get_contrasts")
contrasts <-  contrasts$merged |> prolfqua::ContrastsModerated$new() 

## plot Figure 4 panels A - C
pl <- contrasts$get_Plotter() 
histpval <- pl$histogram()$p.value + ggplot2::labs(tag = "A")               
maplot <- pl$ma_plot(rank = TRUE,legend = FALSE) + ggplot2::labs(tag = "C")
volcano <- pl$volcano()$FDR +
   ggplot2::theme(legend.position = "bottom") +
   ggplot2::labs(tag = "B")

gridExtra::grid.arrange(histpval, maplot, volcano, layout_matrix = rbind(c(1,3),c(2,3)))

```

*R* linear model and linear mixed effect models allow modeling parallel designs, repeated measurements, factorial designs, and many more. Models in *prolfqua* are specified using *R*'s linear model and mixed model formula interface. Therefore, knowledge of the *R* regression model infrastructure [@faraway2016extending; @MASS2002] is advantageous when using our package. Furthermore, this glass box approach should make it easy to reimplement an analysis performed with *prolfqua* using base *R* or other programming languages by reading the analysis script.
However, in the package documentation, we showcase how a user, without this knowledge, can analyze experiments with a parallel group design and a factorial design.

Using the data frame of tidy data ensures interoperability with other proteomics-related packages that manage their data with tidy-tables, e.g., `r BiocStyle::CRANpkg("protti")` [@quast2022protti]. To simplify the integration of *prolfqua* with Bioconductor based workflows there is a method that converts the `LFQData` class into a `r Biocpkg("SummarizedExperiment")`. The use of R6 classes, which encapsulate the configuration and the data, allows for writing very concise code where functions can have few arguments. Auto-completion support for R6 classes in the *RStudio* editor makes it easy for novices to explore *prolfqua*'s functionality (see Supplementary Figure 8).

To ease the usage barriers of the *R* package to users not familiar with statistics and *R* programming, we developed an application based on the *prolfqua* package into our data management platform [B-Fabric](https://fgcz-bfabric.uzh.ch/) [@bfabric;@bfabricjib2022]. The B-Fabric system runs a computing infrastructure controlled by a local resource management system that supports cloud-bursting [@VMMAD]. This integration enables users to select the input data and basic settings in a graphical user interface (GUI). 

In this GUI application, we determine the model formula, and which comparisons to compute from the sample annotation provided in tabular form, similarly to *SAINTexpress* or *MSstats*. However, this application can analyze only parallel group designs with and without repeated measurements or factorial designs without interactions. The user receives a report including input files, the *R* markdown file, and *R* scripts necessary to replicate the analysis using their in-house *R* installation. In this way, *prolfqua* and [B-Fabric](https://fgcz-bfabric.uzh.ch/) help scientists to meet requirements from funding agencies, journals, and academic institutions while publishing their data according to the FAIR  [@FAIR] data principles. We are working on creating a shiny stand-alone application with the described functionality and making it available soon.

## Benchmarking modelling approaches

The Benchmark functionality of *prolfqua* includes receiver operator curves (ROC) and computes partial areas under those curves ($pAUC$) and other scores, e.g., the false discovery proportion $FDP$. We use those scores, i.e. the $pAUC$ at $10\%$ $FDR$ and the $FDP$, to examine how well the methods implemented in *prolfqua* model quantitative mass spectrometric high throughput data and compare them with the results produced by *MSstats*, *proDA* and *msqrob2*. Table \@ref(tab:describeBenchmarkedModels) summarizes all the modelling methods we evaluated.


```{r readBenchmarkData}

getpath <- function(filN){
    f1 <- paste0("../../inst/Benchresults/",filN)
    if (f1 == "") {
        f1 <- system.file("Benchresults",filN, package = "prolfquabenchmark")
    }
    message(f1)
    return(f1)
}
allBenchmarks <- readRDS(getpath("allBenchmarks.RDS"))
benchmark_msstats <- readRDS(getpath("benchmark_msstats.RDS"))
benchmark_prodA <- readRDS(getpath("benchmark_medpolish_proDA.RDS"))
msFragger <- readRDS(getpath("MSFragger_medpol_benchmark.RDS"))
benchmark_msqrob <- readRDS(getpath("benchmark_msqrob.RDS"))

allBenchmarks$benchmark_mssstats <- benchmark_msstats
allBenchmarks$benchmark_msFragger <- msFragger
allBenchmarks$benchmark_proDA <- benchmark_prodA
allBenchmarks$benchmark_msqrob <- benchmark_msqrob

allBenchmarks <- allBenchmarks[c("benchmark_imputation","benchmark_ProtModerated",  "benchmark_mixedModerated", "benchmark_ropeca","benchmark_merged","benchmark_mssstats","benchmark_proDA", "benchmark_msqrob")]

```


```{r benchmarkROC}
ttt <- sapply(allBenchmarks, function(x){x$complete(FALSE)})
for (i in seq_along(allBenchmarks)) {
  allBenchmarks[[i]]$pAUC()
}
res <- purrr::map_df(allBenchmarks, function(x){x$pAUC()})
res <- res |> dplyr::mutate(whatfix = dplyr::case_when(what == "scaled.beta.based.significance" ~ "scaled.p.value", TRUE ~ what))

norm <- res |> dplyr::group_by(contrast, whatfix) |>
    dplyr::summarize(meanpAUC_10 = mean(pAUC_10))
res <- dplyr::inner_join(res, norm)
res <- dplyr::mutate(res , pAUC_10n = pAUC_10 - meanpAUC_10)

resAllB <- res |> dplyr::filter(contrast == "all")

pB_pAUC <- ggplot2::ggplot(resAllB, ggplot2::aes(x = Name, y = pAUC_10)) +
  ggplot2::geom_bar(stat = "identity") +
  ggplot2::facet_wrap(~whatfix)  + 
  ggplot2::coord_cartesian(ylim = c(min(resAllB$pAUC_10),max(resAllB$pAUC_10))) + 
  ggplot2::theme_minimal() + 
  ggplot2::theme(axis.text.x = ggplot2::element_text(angle = -90, vjust = 0.5)) +
  ggplot2::geom_hline(ggplot2::aes(yintercept = meanpAUC_10), color = "red") +
  ggplot2::xlab("") +
  ggplot2::labs(tag = "B")

p2_compare_variousLevels <- ggplot2::ggplot(res, ggplot2::aes(x = contrast, y = pAUC_10n, group = Name)) +
  ggplot2::geom_line(stat = "identity", ggplot2::aes(linetype = Name, color = Name), size = 1) + 
  ggplot2::facet_wrap(~ whatfix, scales = "free") +
  ggplot2::theme_minimal() + 
  ggplot2::theme(axis.text.x = ggplot2::element_text(angle = -90, vjust = 0.5)) +
  ggplot2::geom_hline(ggplot2::aes(yintercept = 0), color = "red") + 
  ggplot2::theme(legend.position = "bottom", legend.title = ggplot2::element_blank())

```

When comparing DEA performance, a relevant parameter is the number of proteins for which a method estimates differences (see Figure \@ref(fig:figure3) Panel A), which indicates how robust the procedure works in the presence of missing observations. For each protein, we tried to determine four differences ($\Delta = (1.20, 1.25, 1.30, 1.50)$). Given 4178 proteins with at least two peptides, there are in total $16712$ possible differences. However, some methods can not estimate all of them. The set of proteins with effect size estimates might differ for each method, biasing direct comparison of scores such as the $pAUC$. For instance, we observe that *MSstats* estimates $16038$ group differences while the mixed effect models estimates the fewest with $15756$. Hence, to conclude that one method shows a better performance, it does not suffice if the $pAUC$ is greater, but the number of proteins with differential expression results needs to be equal or larger.

Figure \@ref(fig:figure3) Panel B shows how various estimates obtained from the models, i.e., the difference between groups, $t$-statistics and scaled $p$-values, allow identifying true positives (TP) given a false positive rate (FPR) of $10\%$ by displaying the partial area under the ROC ($pAUC_{10}$). Ordering proteins by the $t$-statistic or $p$-value leads to a higher $pAUC_{10}$ than when ordering by the estimated difference among groups. Suppose an accurate estimate of the difference among groups is essential. In that case, the linear models fitted to protein intensities, calculated using Tukey's median polish, perform better than those when directly modeling peptide intensities, i.e., `ropeca` (see Table \@ref(tab:describeBenchmarkedModels) Abundance column). We speculate that outliers at the peptide level do not affect the protein estimates when using Tukey's median polish method.

We also benchmark if the $FDR$ obtained from a model is an unbiased estimate of the false discovery proportion $FDP$. Figure \@ref(fig:figure3) Panel C shows the $FDP$, obtained from the confusion matrix, as a function of the $FDR$ determined from the model. Most lines are below the diagonal, which indicates that the $FDR$ estimates are modestly conservative for this particular benchmark dataset. In the case of *MSstats*, we observe a high proportion of false discoveries for small $FDR$ values. In the case of the ROPECA method, the $FDR$ estimates, obtained by applying the Benjamini-Hochberg correction to the Beta distribution-based regulation probabilities, strongly overestimate the $FDP$.

(ref:figure3) Panel A - Number of estimated contrasts for each modeling method (higher is better). Panel B - Partial area under the ROC curve at $10\%$ FPR ($pAUC_{10}$) for all contrasts and three different statistics: the difference among groups (diff - Panel B left), the scaled $p$-value (sign(diff) $\cdot$ p.value) (scaled.p.value - Panel B center) and the $t$-statistics (statistic - Panel B right), where a higher $pAUC_{10}$ is better. The red line indicates the average area under the curve of all methods.  Panel C - Plots the false discovery proportion ($FDP$) as a function of the $FDR$. Ideally, the $FDR$ should be equal to the $FDP$. Therefore larger distances from the diagonal are worse.

```{r figure3, fig.cap = "(ref:figure3)", out.width = '90%', fig.width=10, fig.height=8}
dd <- purrr::map_df(allBenchmarks, function(x){res <- x$smc$summary; res$name <- x$model_name;res})
dd <- dd |> dplyr::mutate(nrcontrasts = protein_Id * (4 - nr_missing))
dds <- dd |> dplyr::group_by(name) |> dplyr::summarize(nrcontrasts = sum(nrcontrasts))
dds$percent <- dds$nrcontrasts/max(dds$nrcontrasts) * 100

nrgg <- dds |> ggplot2::ggplot(ggplot2::aes(x = name, y = nrcontrasts )) + 
  ggplot2::geom_bar(stat = "identity", fill = "white", colour = "black") + 
  ggplot2::coord_cartesian(ylim = c(min(dds$nrcontrasts) - 200, max(dds$nrcontrasts) + 10)) +
  ggplot2::theme(axis.text.x = ggplot2::element_text(angle = -90, vjust = 0.5)) +
  ggplot2::geom_text(ggplot2::aes(label = paste0(round(nrcontrasts, digits = 1), paste0("  (",round(percent, digits = 1),"%)"))),
            vjust = 0, hjust = -0.2, angle = -90) #+ 
pA_barplot <- nrgg + ggplot2::labs(tag = "A")

dd <- purrr::map_df(allBenchmarks, function(x){res <- x$get_confusion_FDRvsFDP(); res$name <- x$model_name;res})

ddb <- dplyr::filter(dd, contrast == "dilution_(4.5/3)_1.5")
ddb <- dd |> dplyr::filter(contrast == "dilution_(7.5/6)_1.25")
ddb <- dd |> dplyr::filter(contrast == "all")
ddb$FDP <- NULL
ddb <- ddb |> dplyr::rename(FDR = scorecol, FDP = FDP_)

pC_FDR <- ddb |> ggplot2::ggplot(ggplot2::aes(y = FDP,  x  = FDR )) + 
  ggplot2::geom_line(ggplot2::aes(color = model_name, linetype = model_name)) +
  ggplot2::facet_wrap(~contrast) + 
  ggplot2::geom_abline(intercept = 0, slope = 1, color = 2) + 
  ggplot2::theme(legend.position = "bottom") +
  ggplot2::labs(tag = "C") + ggplot2::xlim(c(0,0.5)) + ggplot2::ylim(c(0,0.5))
hlay = rbind(c(2,1,1),
            c(2,3,3))

gridExtra::grid.arrange(pB_pAUC, pA_barplot, pC_FDR, layout_matrix = hlay)

```



Using a benchmark dataset whose ground truth is known (see Methods), we assessed the performance of different modeling approaches implemented in *prolfqua*, *MSstats*, *proDA* and *msqrob2*. Table \@ref(tab:describeBenchmarkedModels) summarizes which methods we have evaluated, which *MaxQuant* files we used as input, and if the models are fitted to peptide or protein intensities. We make the *R*-markdown files to replicate the benchmarking available at `r BiocStyle::Githubpkg("wolski/prolfquabenchmark")`.


```{r describeBenchmarkedModels, results='asis' }
xMM <- data.frame(
    msstats = c("MSstats", "preprocess with default parameters",  "", "evidence.txt"),
    msqrob = c("msqrob2", "hurdle msqrob model", "protein and peptide", "peptide.txt"),
    proDA = c("proDA", "probabilistic dropout model",  "protein", "peptide.txt"),
    imputation = c("prolfqua_impute", "ContrastsSimpleImpute, ContrastsModerated", "protein",  "peptide.txt"),
    lmmed = c("prolfqua_lm_mod", "strategy_lm, Contrasts, ContrastsModerated",  "protein", "peptide.txt"),
    prot_merged = c("prolfqua_merged", "addContrastResults( prefer = prot_med_lm_moderated , add = prot_imputation)HACKIT ",  "protein", "peptide.txt"),
    mixed = c("prolfqua_mix_eff_mod", "strategy_lmer, Contrasts, ContrastsModerated", "peptide", "peptide.txt" ),
    ropeca = c("prolfqua_ropeca", "strategy_lm, Contrasts, ContrastsModerated, ContrastsROPECA", "peptide", "peptide.txt" )
)

xMM <- data.frame(t(xMM))
rownames(xMM) <- NULL
colnames(xMM) <-  c("Label","Description","Abundance","Input File")
tmp <- kableExtra::kable(xMM,
             caption = paste0("All benchmarked models. ",
             "Description - prolfqua function names, ",
             "Abudances - indicates if model is fitted to peptide or protein abundances, ",
             "Input File - name of MaxQuant file used as input."),
             format = "latex", booktabs = TRUE) |>
    kableExtra::kable_styling(latex_options = "scale_down") |> 
    kableExtra::add_footnote(label = "'prolfqua_merged' - Augments estimates which are missing in 'prot_med_lm_mod' with those from 'prot_impute'.")


knitr::asis_output(stringr::str_replace(tmp, "HACKIT", "$^{a}$"))
saveRDS(allBenchmarks, "allBenchmarks.RDS")

```


 
The IonStar/MaxQuant dataset (see Methods) captures only the variance from the chromatography, electro-spray, and mass spectrometric measurements since only technical replicates are available for each dilution. Therefore essential sources of variation typically present in other experiments, such as the biochemical and biological ones, are not measured. Furthermore, this dataset with a parallel group design does not allow for benchmarking models with interactions. Thus, while we can extrapolate some of the results obtained to more realistic datasets, we need to be careful not to over-interpret our findings. Specifically, the observed variances will be higher in datasets with biological replicates, and the power will be lower for the same number of samples. Furthermore, the proportion of missing observations in real-life datasets might be higher or distributed differently in groups.

We can conclude that if we want to sort the proteins according to the likelihood of being differentially regulated to perform gene set enrichment analysis [@subramanian2005gene], the $t$-statistic is better suited than the fold-change estimate. When computing the $p$-values from the t-statistics we are incorporating the degrees of freedom, which improve the inferences (see Figure \@ref(fig:figure3) panel B center versus panel B left). There is no such improvement for the mixed effect model. The reason is an erroneous denominator degree of freedom estimation for many proteins, a known problem in the case of mixed effect models. Furthermore, for the fixed effect linear model, the empirical Bayes variance shrinkage, as suggested by @Smyth2004linear, consistently improves the ranking of proteins compared with the unmoderated estimates (not shown). However, since also for this method a correct degree of freedom estimate is required, it does not work for mixed effect models.

Computing the statistics at the peptide level, e.g., the $t$-statistics, then summarizing it for each protein using their median, produces the highest $pAUC_{10}$ scores among all the tested models (see Figure \@ref(fig:figure3) Panel B `prolfqua_ropeca`). Furthermore, by using the Beta distribution to model the number of peptides observed, we can further improve the $pAUC$ scores (see Figure \@ref(fig:figure3) Panel B center). However, the properties of Beta-based probabilities are not well understood; for instance, it is not uniformly distributed under the null hypothesis (see Supplementary Section "The probabilities produced by ROPECA are not p- values"). Furthermore, the $FDR$ estimates obtained when correcting for multiple testing with the Benjamini-Hochberg method are biased and overestimate the FDP (see Figure \@ref(fig:figure3) Panel C). Therefore, we can not recommend this method if an unbiased estimate of $FDR$ is essential, which is frequently the case. In addition, peptides are stronger affected by missing values, reducing the number of contrasts we could estimate for the dataset using this method (see Figure \@ref(fig:figure3) Panel C).

The *R*-package *proDA*, *msqrob2* and *prolfqua* do not impute missing data but integrate them into the statistical model, while *MSstats* filters and imputes the data using an accelerated failure model. Despite imputation, *MSstats* estimates group differences for less proteins, and does not achieve a higher $pAUC_{10}$. Furthermore, Figure \@ref(fig:figure3) Panel C shows that when using *MSstats* the proportion of false discoveries might be very high for a low $FDR$ because of false positives. Of note, *MSStats* uses the _evidence.txt_ file, while all the other methods use _peptide.txt_ files, as input (see Table \@ref(tab:describeBenchmarkedModels))). Furthermore, *MSstats* uses equalized medians normalization, while all the other methods use robust scaling (see Methods). These are possible confounding factors to consider. While *prolfqua*, as well as *proDA*, is highly modular, and to a lesser extent *msqrob2*, enabling us to use the same data preprocessing and normalization, *MSstats* is a monolithic software, making it unfeasible to change the preprocessing or normalization. Therefore, we could not control these confounders.

We obtained difference and FDR estimates for all proteins, as shown in Figure \@ref(fig:figure3) Panel A when using: a) the probabilistic dropout model (*proDA*), b) the hurdle model (*msqrob2*), and c) 'prolfqua_merge' (Table \@ref(tab:describeBenchmarkedModels). We observe that the performance of the scaled $p$-values or the $t$-statistics is comparable among these three methods (Figure \@ref(fig:figure3) Panel B). We tested if there is a significant difference between the $pAUC_{10}$ for all three methods, but did not reject the null hypothesis that there is no such difference (Supplementary Section "DEA benchmark : IonStar/MaxQuant/peptide.txt - Significance test"). Also, the FDR estimates (Figure \@ref(fig:figure3) Panel C) are comparable for all three methods. 

Furthermore, all three models perform similarly when examined using a different benchmark dataset CPTAC/MaxQuant (see Methods). For this dataset, *proDA* performed slightly but not significantly better than *prolfqua* and *msqrob* (and Supplementary Section "DEA benchmark : CPTAC/MaxQuant/peptide.txt").

In addition, we examined the DEA performance when using protein intensities reported by quantification software *FragPipe* for the *IonStar* dataset as input. Using protein abundances as input significantly simplifies the analysis and interpretation and might benefit from optimization implemented in the quantification software. However, we can only easily fit the *proDA* and 'prolfqua_merged' (see Table \@ref(tab:describeBenchmarkedModels)) models to protein intensities. In this DEA benchmark, *prolfqua* performed slightly, but not significantly better than *proDA* (Supplementary Section "DEA benchmark : IonStar/FragPipeV14/combined_protein.tsv").

Finally, we also compared the DEA performances when starting the analysis from the precursor abundances reported in the 'MSstats.tsv' file, generated by *FragPipe* v14, from the *IonStar* dataset. Since *MSstats*, *msqrob2*, *proDA*, and *prolfqua* all read `MStstats.tsv` files, we could eliminate a confounding factor, i.e., different input abundances (Supplementary Section "DEA benchmark: IonStar/FragPipeV14/MSstats.tsv"). In this DEA benchmark, *msqrob2* and *prolfqua* ('prolfqua_merged') perform best but not significantly better than *proDA* or *MSstats*. Furthermore, by processing the *IonStar* dataset with both *MaxQuant* and *FragPipe*, we can compare their performances (Supplementary Section "Comparing DEA results for *MaxQuant* and *FragPipe*").

We focused our benchmark on comparing the statistical modeling methods while we fixed the pre-processing steps. However, some of these steps are of utmost significance when performing differential expression analysis [@frohlich2022benchmarking]. One of them is the normalization of the intensities within the samples to remove systematic differences [@pursiheimo2015optimization]. The method used to infer proteins from peptide identifications [@nesvizhskii2005interpretation] and protein abundances from peptide abundances is an additional important factor [@Grossmann2010]. For instance, the original *proDA* publication uses MaxLFQ [@cox2014accurate] protein estimates. However, when using MaxLFQ intensities reported by *MaxQuant*, the $pAUC_{10}$ is lower ($pAUC_{10}(\textrm{t-statistics}) = 66\%$) compared with results obtained when protein abundances are estimated from peptide abundances using Tukey's median polish ($pAUC_{10}(\textrm{t-statistics}) = 72\%$). Last but not least, the software [@Cox2008MaxQuant; @yu2020fast] used to identify and quantify proteins significantly contributes to the entire pipeline's performance altering the number of identified proteins and the sensitivity and specificity of the differential expression analysis. In the Supplementary Section "Comparing DEA results for *MaxQuant* and *FragPipe*" we compare DEA benchmarking results for the quantification software. While the number of proteins identified with two peptides is practically the same, the DEA benchmark performance differs significantly by $\approx 10\%$ for the $pAUC_{10}$ score. This difference is more significant than the differences due to the choice of the modeling method.

# Conclusion

`r BiocStyle::Githubpkg("fgcz/prolfqua")` is a feature-rich, object-oriented, and modular *R* package to analyze quantitative mass spectrometric data with simple or complex experimental designs. While other *R* packages for differential expression analysis of proteins typically only implement one modeling approach, *prolfqua* supports various types of models (see Figure \@ref(fig:ContrastUML) and Table \@ref(tab:prolfquaModels)). Furthermore, the contrast specification is explicit and consistent for all models and allows for testing interactions. The modular design of *prolfqua* enables adding new features, e.g., generalized linear models to model the presence or absence information of a protein, or robust linear models, in the future. Furthermore, the developed framework can integrate other modeling methods, e.g., the probabilistic dropout model [@bioxrvproDA2020] or accurate variance estimation [@zhu2020deqms]. Hence, *prolfqua* enables the implementation of applications where the user can select an alternative normalization method, protein abundance estimation methods, or DEA algorithms. Furthermore, this *R* package can analyze other types of quantitative proteomics data, e.g., label-free DIA or labeling-based TMT data.

When comparing statistical modeling methods for the DEA, we assessed performance measures such as the number of estimated contrasts, the $pAUC$, and if the $FDR$ is an unbiased estimate of the $FDP$. It is relevant that an analysis pipeline shows good performance in all these categories. Leveraging these computational experiments, we can provide the following advice: i) estimate protein abundances from peptide abundances using a robust or nonparametric regression method; ii) use linear models because they show good performance in all categories; iii) if the measurements are correlated, as for technical replicates, mixed effect models might work if the sample sizes are large; if not, aggregate the replicates and fit a linear model instead; iv) if you use fixed-effect linear models, apply variance moderation to improve the $t$-statistics and $p$-value estimates; v) If you want to sort your protein lists to perform gene set enrichment analysis, use the $t$-statistic instead of the difference; and last but not least vi) do not impute missing observation but statistically model missingness to estimate parameters, i.e., group differences. Finally, the DEA results obtained with *prolfqua* are comparable to those of other DEA tools. 


# Acknowledgements {.unlisted .unnumbered}

The authors thank the technology platform fund of the University of Zurich and all colleagues of the FGCZ proteomics team for fruitful discussions.

# Abbreviations {.unlisted .unnumbered}

\begin{table}
\centering
\begin{tabular}{ll}
\toprule
Abbreviations & Explanation\\
\midrule
API    & application programming interface          \\
AUC    & Area Under the Curve                               \\
CDF    & Cumulative Distribution Function                   \\
DEA & Differential Expression Analysis \\
DIA    & data independent acquisition                        \\
ESI-MS & Electro-Spray-Ionization Mass Spectrometry         \\
FAIR   & Findable, Accessible, Interoperable, and Reusable  \\
$FDP$  & False Discovery Proportion                         \\
$FDR$  & False Discovery Rate                               \\
$FP$ & False Postive \\
$FPR$ & False Positive Rage    \\
LC     & Liquid Chromatography                              \\
LC-MS  & Liquid Chromatography followed by Mass Spectrometry\\
LOD    & Limit Of Detection                                 \\
MAR    & Missing At Random                                 \\ 
MCAR   & Missing Completely At Random                      \\
ML     & machine learning                                 \\
MS     & mass spectrometry                                  \\
pAUC   & partial Area Under the Curve                       \\
$pAUC_{10}$ & partial Area Under the Curve for an FPR of 0 to $10\%$ \\
ROC    & Receiver Operator Curve                           \\
TP & True Positive \\
TMT    & Tandem Mass Tag                                   \\
UML    & Unified Modeling Language                        \\
\bottomrule
\end{tabular}
\end{table}

\newpage

# References {-}

<div id="refs"></div>

\newpage

# Appendix  {.unnumbered}


## Session information {.unlisted .unnumbered}

```{r sessioninfo, echo=FALSE}
pander::pander(sessionInfo())
```


```{r proLFQuaSticker, echo=TRUE, out.height="5cm", eval=TRUE, fig.cap="Sticker maintainer: Witold E. Wolski; License: Creative Commons Attribution CC-BY. Feel free to share and adapt, but don't forget to credit the author."}

file.path("hexStickerProlfqua.png") |>
  knitr::include_graphics()
```


